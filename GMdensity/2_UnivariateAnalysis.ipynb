{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "from nipype.interfaces.fsl.utils import Merge, Split, Smooth\n",
    "from nipype.interfaces.fsl.model import Randomise, Cluster, GLM\n",
    "from nipype.interfaces.freesurfer import FSCommand\n",
    "from pandas import read_csv\n",
    "\n",
    "# FSL set up- change default file output type\n",
    "from nipype.interfaces.fsl import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI_GZ')\n",
    "\n",
    "#freesurfer setup\n",
    "fs_dir = '/moochie/Cat/Aggregate_anats/subjects_dir'\n",
    "FSCommand.set_default_subjects_dir(fs_dir)\n",
    "\n",
    "#other study-specific variables\n",
    "project_home = '/home/camachocm2/Analysis/local_moochie/Aggregate_anats/GMD_ML/ml_trainingset/smooth4'\n",
    "workflow_dir = project_home + '/LMs/workflows'\n",
    "merged_infile = project_home + '/gmd_combined_4.nii.gz'\n",
    "output_dir = project_home + '/LMs/results'\n",
    "template = '/moochie/user_data/CamachoCat/Aggregate_anats/GMD_ML/templates/lcbd_template_1mm.nii.gz'\n",
    "mask =  project_home + '/age_agg_mask.nii.gz'\n",
    "\n",
    "# Files for group level analysis\n",
    "t_contrasts = project_home + '/misc/tcon.con'\n",
    "design_file = \n",
    "subject_info = read_csv(project_home + '/featureset_key.csv', index_col=0)\n",
    "subjects_list = subject_info['freesurferID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling Nodes\n",
    "\n",
    "datasink = Node(DataSink(), name='datasink')\n",
    "datasink.inputs.base_directory = output_dir\n",
    "datasink.inputs.container = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_peaks(clusters_file, stat_file):\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, unravel_index, max\n",
    "    from os.path import abspath\n",
    "    \n",
    "    # load up clusters\n",
    "    clusters_nii = load(clusters_file)\n",
    "    clusters_data = clusters_nii.get_data()\n",
    "    cluster_labels, cluster_sizes = unique(clusters_data, return_counts=True)\n",
    "    cluster_sizes = cluster_sizes[cluster_labels>0]\n",
    "    cluster_labels = cluster_labels[cluster_labels>0]\n",
    "    \n",
    "    # set up dataframe\n",
    "    cluster_info = DataFrame(columns=['clust_num','peak','num_voxels','X','Y','Z'])\n",
    "    cluster_info['clust_num'] = Series(cluster_labels,index=None)\n",
    "    \n",
    "    for i in range(0,len(cluster_labels)):\n",
    "        # load up stat image\n",
    "        stat_nii = load(stat_file)\n",
    "        stat_data = stat_nii.get_data()\n",
    "        stat_data[clusters_data!=cluster_labels[i]]=0\n",
    "        location=unravel_index(stat_data.argmax(), stat_data.shape)\n",
    "        cluster_info.iloc[i,0]=cluster_labels[i]\n",
    "        cluster_info.iloc[i,1]=max(stat_data)\n",
    "        cluster_info.iloc[i,2]=cluster_sizes[i]\n",
    "        cluster_info.iloc[i,3]=location[0]\n",
    "        cluster_info.iloc[i,4]=location[1]\n",
    "        cluster_info.iloc[i,5]=location[2]\n",
    "    \n",
    "    out_prefix = clusters_file[:-7]\n",
    "    cluster_info.to_csv(out_prefix + '_peaks.csv')\n",
    "    cluster_info_file = abspath(out_prefix + '_peaks.csv')\n",
    "    return(cluster_info_file)\n",
    "\n",
    "def extract_cluster_betas(cluster_index_file, sample_L1_data, min_clust_size, subject_ids):\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, zeros_like, invert\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    from os.path import abspath, basename\n",
    "    \n",
    "    sample_data = DataFrame(subject_ids, index=None, columns=['Subject'])\n",
    "    \n",
    "    cluster_nifti = load(cluster_index_file)\n",
    "    cluster_data = cluster_nifti.get_data()\n",
    "    clusters, cluster_sizes = unique(cluster_data, return_counts=True)\n",
    "    cluster_sizes = cluster_sizes[clusters>0]\n",
    "    clusters = clusters[clusters>0]\n",
    "    clusters = clusters[cluster_sizes>min_clust_size]\n",
    "    cluster_sizes = cluster_sizes[cluster_sizes>min_clust_size]\n",
    "    ind_filename = basename(cluster_index_file) \n",
    "    out_prefix = ind_filename[:-7]\n",
    "    \n",
    "    for clust_idx in clusters:\n",
    "        temp = zeros_like(cluster_data)\n",
    "        temp[cluster_data==clust_idx] = 1\n",
    "        temp_nii = Nifti1Image(temp,cluster_nifti.affine)\n",
    "        temp_file = 'temp_clust_mask.nii.gz'\n",
    "        save(temp_nii, temp_file)\n",
    "\n",
    "        eb = ImageMeants()\n",
    "        eb.inputs.in_file = sample_L1_data\n",
    "        eb.inputs.mask = temp_file\n",
    "        eb.inputs.out_file = 'L1vals.txt'\n",
    "        eb.run()\n",
    "        L1vals = open('L1vals.txt').read().splitlines()\n",
    "        sample_data['clust' + str(clust_idx)] = Series(L1vals, index=sample_data.index)\n",
    "    \n",
    "    sample_data.to_csv(out_prefix+'_extracted_L1vals.csv')\n",
    "    extracted_L1vals_csv = abspath(out_prefix+'_extracted_L1vals.csv')\n",
    "\n",
    "    return(extracted_L1vals_csv)\n",
    "\n",
    "def calculate_tstat_min(subject_data_list):\n",
    "    from scipy.stats import t\n",
    "    \n",
    "    N_subs = len(subject_data_list)\n",
    "    dof = N_subs - 1\n",
    "    tstat_threshold = t.ppf(1-0.001, dof)\n",
    "    \n",
    "    return(tstat_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Nodes\n",
    "\n",
    "model = Node(Randomise(mask=mask,\n",
    "                       in_file=merged_infile,\n",
    "                       tfce=False,\n",
    "                       num_perm=1000,\n",
    "                       tcon=t_contrasts), \n",
    "             name='model')\n",
    "\n",
    "split = Node(Split(dimension='t'), name='split')\n",
    "\n",
    "calc_threshold = Node(Function(input_names=['subject_data_list'], \n",
    "                               output_names=['tstat_threshold'], \n",
    "                               function=calculate_tstat_min), \n",
    "                      name='calc_threshold')\n",
    "\n",
    "cluster = MapNode(Cluster(out_localmax_txt_file = 'cluster_stats.txt',\n",
    "                          dlh=4, \n",
    "                          pthreshold=0.05, \n",
    "                          volume=1472512,\n",
    "                          out_index_file='clusters.nii.gz'), \n",
    "                  name='cluster', iterfield=['in_file'])\n",
    "\n",
    "get_peaks = MapNode(Function(input_names=['clusters_file', 'stat_file'], \n",
    "                             output_names=['cluster_info_file'], \n",
    "                             function=get_cluster_peaks), \n",
    "                    name='get_peaks', iterfield=['clusters_file', 'stat_file'])\n",
    "\n",
    "get_L1vals = MapNode(Function(input_names=['cluster_index_file', 'sample_L1_data', \n",
    "                                           'min_clust_size', 'subject_ids'], \n",
    "                              output_names=['extracted_L1vals_csv'], \n",
    "                              function=extract_cluster_betas), \n",
    "                     name='get_L1vals', iterfield=['cluster_index_file'])\n",
    "get_L1vals.inputs.min_clust_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190513-14:52:36,193 workflow INFO:\n",
      "\t Generated workflow graph: /home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/graph.png (graph2use=flat, simple_form=True).\n",
      "190513-14:52:36,246 workflow INFO:\n",
      "\t Workflow grouplevel_sescomposite settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "190513-14:52:36,292 workflow INFO:\n",
      "\t Running in parallel.\n",
      "190513-14:52:36,299 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-14:52:36,392 workflow INFO:\n",
      "\t [Node] Setting-up \"grouplevel_sescomposite.grabdata\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/grabdata\".\n",
      "190513-14:52:36,406 workflow INFO:\n",
      "\t [Node] Running \"grabdata\" (\"nipype.interfaces.io.DataGrabber\")\n",
      "190513-14:52:36,769 workflow INFO:\n",
      "\t [Node] Finished \"grouplevel_sescomposite.grabdata\".\n",
      "190513-14:52:38,300 workflow INFO:\n",
      "\t [Job 0] Completed (grouplevel_sescomposite.grabdata).\n",
      "190513-14:52:38,305 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-14:52:38,943 workflow INFO:\n",
      "\t [Job 1] Cached (grouplevel_sescomposite.select_usable_data).\n",
      "190513-14:52:40,302 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 2 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-14:52:40,380 workflow INFO:\n",
      "\t [Job 2] Cached (grouplevel_sescomposite.calc_threshold).\n",
      "190513-14:52:40,400 workflow INFO:\n",
      "\t [Node] Setting-up \"grouplevel_sescomposite.model\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/model\".\n",
      "190513-14:52:40,412 workflow INFO:\n",
      "\t [Node] Running \"model\" (\"nipype.interfaces.fsl.model.Randomise\"), a CommandLine Interface with command:\n",
      "randomise -i /home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/select_usable_data/data_merged.nii.gz -o \"randomise\" -d /home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/select_usable_data/design.mat -t /home/camachocm2/Analysis/aggregate_anats/misc/tcon_income.con -m /moochie/Cat/Aggregate_anats/GMD_ML/templates/lcbd_template_1mm_mask.nii.gz -n 1000\n",
      "190513-14:52:42,305 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * grouplevel_sescomposite.model\n",
      "190513-14:55:32,533 workflow INFO:\n",
      "\t [Node] Finished \"grouplevel_sescomposite.model\".\n",
      "190513-14:55:34,483 workflow INFO:\n",
      "\t [Job 3] Completed (grouplevel_sescomposite.model).\n",
      "190513-14:55:34,488 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-14:55:36,486 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 4 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-14:55:36,583 workflow INFO:\n",
      "\t [Node] Setting-up \"_cluster0\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster/mapflow/_cluster0\".190513-14:55:36,584 workflow INFO:\n",
      "\t [Node] Setting-up \"_cluster1\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster/mapflow/_cluster1\".\n",
      "\n",
      "190513-14:55:36,593 workflow INFO:\n",
      "\t [Node] Running \"_cluster1\" (\"nipype.interfaces.fsl.model.Cluster\"), a CommandLine Interface with command:\n",
      "cluster --dlh=4.0000000000 --in=/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/model/randomise_tstat3.nii.gz --oindex=clusters.nii.gz --olmax=cluster_stats.txt --pthresh=0.0500000000 --thresh=3.1589544051 --volume=1472512190513-14:55:36,593 workflow INFO:\n",
      "\t [Node] Running \"_cluster0\" (\"nipype.interfaces.fsl.model.Cluster\"), a CommandLine Interface with command:\n",
      "cluster --dlh=4.0000000000 --in=/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/model/randomise_tstat1.nii.gz --oindex=clusters.nii.gz --olmax=cluster_stats.txt --pthresh=0.0500000000 --thresh=3.1589544051 --volume=1472512\n",
      "\n",
      "190513-14:55:38,489 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 2 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _cluster1\n",
      "                       * _cluster0\n",
      "190513-14:55:41,667 workflow INFO:\n",
      "\t [Node] Finished \"_cluster0\".\n",
      "190513-14:55:42,491 workflow INFO:\n",
      "\t [Job 8] Completed (_cluster0).\n",
      "190513-14:55:42,495 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 2 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _cluster1\n",
      "190513-14:55:42,586 workflow INFO:\n",
      "\t [Node] Setting-up \"_cluster2\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster/mapflow/_cluster2\".\n",
      "190513-14:55:42,595 workflow INFO:\n",
      "\t [Node] Running \"_cluster2\" (\"nipype.interfaces.fsl.model.Cluster\"), a CommandLine Interface with command:\n",
      "cluster --dlh=4.0000000000 --in=/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/model/randomise_tstat2.nii.gz --oindex=clusters.nii.gz --olmax=cluster_stats.txt --pthresh=0.0500000000 --thresh=3.1589544051 --volume=1472512\n",
      "190513-14:55:43,464 workflow INFO:\n",
      "\t [Node] Finished \"_cluster1\".\n",
      "190513-14:55:44,493 workflow INFO:\n",
      "\t [Job 9] Completed (_cluster1).\n",
      "190513-14:55:44,497 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 1 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _cluster2\n",
      "190513-14:55:44,567 workflow INFO:\n",
      "\t [Node] Setting-up \"_cluster3\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster/mapflow/_cluster3\".\n",
      "190513-14:55:44,576 workflow INFO:\n",
      "\t [Node] Running \"_cluster3\" (\"nipype.interfaces.fsl.model.Cluster\"), a CommandLine Interface with command:\n",
      "cluster --dlh=4.0000000000 --in=/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/model/randomise_tstat4.nii.gz --oindex=clusters.nii.gz --olmax=cluster_stats.txt --pthresh=0.0500000000 --thresh=3.1589544051 --volume=1472512\n",
      "190513-14:55:46,496 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 0 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _cluster3\n",
      "                       * _cluster2\n",
      "190513-14:55:48,292 workflow INFO:\n",
      "\t [Node] Finished \"_cluster2\".\n",
      "190513-14:55:48,497 workflow INFO:\n",
      "\t [Job 10] Completed (_cluster2).\n",
      "190513-14:55:48,500 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _cluster3\n",
      "190513-14:55:59,605 workflow INFO:\n",
      "\t [Node] Finished \"_cluster3\".\n",
      "190513-14:56:00,510 workflow INFO:\n",
      "\t [Job 11] Completed (_cluster3).\n",
      "190513-14:56:00,514 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-14:56:00,598 workflow INFO:\n",
      "\t [Node] Setting-up \"grouplevel_sescomposite.cluster\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster\".\n",
      "190513-14:56:00,607 workflow INFO:\n",
      "\t [Node] Setting-up \"_cluster0\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster/mapflow/_cluster0\".\n",
      "190513-14:56:00,612 workflow INFO:\n",
      "\t [Node] Cached \"_cluster0\" - collecting precomputed outputs\n",
      "190513-14:56:00,614 workflow INFO:\n",
      "\t [Node] \"_cluster0\" found cached.\n",
      "190513-14:56:00,620 workflow INFO:\n",
      "\t [Node] Setting-up \"_cluster1\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster/mapflow/_cluster1\".\n",
      "190513-14:56:00,625 workflow INFO:\n",
      "\t [Node] Cached \"_cluster1\" - collecting precomputed outputs\n",
      "190513-14:56:00,628 workflow INFO:\n",
      "\t [Node] \"_cluster1\" found cached.\n",
      "190513-14:56:00,633 workflow INFO:\n",
      "\t [Node] Setting-up \"_cluster2\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster/mapflow/_cluster2\".\n",
      "190513-14:56:00,637 workflow INFO:\n",
      "\t [Node] Cached \"_cluster2\" - collecting precomputed outputs\n",
      "190513-14:56:00,639 workflow INFO:\n",
      "\t [Node] \"_cluster2\" found cached.\n",
      "190513-14:56:00,643 workflow INFO:\n",
      "\t [Node] Setting-up \"_cluster3\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/cluster/mapflow/_cluster3\".\n",
      "190513-14:56:00,646 workflow INFO:\n",
      "\t [Node] Cached \"_cluster3\" - collecting precomputed outputs\n",
      "190513-14:56:00,648 workflow INFO:\n",
      "\t [Node] \"_cluster3\" found cached.\n",
      "190513-14:56:00,662 workflow INFO:\n",
      "\t [Node] Finished \"grouplevel_sescomposite.cluster\".\n",
      "190513-14:56:02,512 workflow INFO:\n",
      "\t [Job 4] Completed (grouplevel_sescomposite.cluster).\n",
      "190513-14:56:02,518 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 2 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-14:56:04,515 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 8 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-14:56:04,599 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_peaks0\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks/mapflow/_get_peaks0\".\n",
      "190513-14:56:04,600 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_peaks1\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks/mapflow/_get_peaks1\".\n",
      "190513-14:56:04,606 workflow INFO:\n",
      "\t [Node] Running \"_get_peaks0\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190513-14:56:04,607 workflow INFO:\n",
      "\t [Node] Running \"_get_peaks1\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190513-14:56:06,517 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 6 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _get_peaks1\n",
      "                       * _get_peaks0\n",
      "190513-14:56:37,55 workflow INFO:\n",
      "\t [Node] Finished \"_get_peaks0\".\n",
      "190513-14:56:38,550 workflow INFO:\n",
      "\t [Job 12] Completed (_get_peaks0).\n",
      "190513-14:56:38,554 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 6 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _get_peaks1\n",
      "190513-14:56:38,634 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_peaks2\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks/mapflow/_get_peaks2\".\n",
      "190513-14:56:38,642 workflow INFO:\n",
      "\t [Node] Running \"_get_peaks2\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190513-14:56:40,554 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 5 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _get_peaks2\n",
      "                       * _get_peaks1\n",
      "190513-14:56:52,324 workflow INFO:\n",
      "\t [Node] Finished \"_get_peaks1\".\n",
      "190513-14:56:52,565 workflow INFO:\n",
      "\t [Job 13] Completed (_get_peaks1).\n",
      "190513-14:56:52,569 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 5 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _get_peaks2\n",
      "190513-14:56:52,651 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_peaks3\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks/mapflow/_get_peaks3\".\n",
      "190513-14:56:52,658 workflow INFO:\n",
      "\t [Node] Running \"_get_peaks3\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190513-14:56:54,569 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 4 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _get_peaks3\n",
      "                       * _get_peaks2\n",
      "190513-14:57:20,149 workflow INFO:\n",
      "\t [Node] Finished \"_get_peaks2\".\n",
      "190513-14:57:20,594 workflow INFO:\n",
      "\t [Job 14] Completed (_get_peaks2).\n",
      "190513-14:57:20,599 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 4 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _get_peaks3\n",
      "190513-14:57:20,702 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_L1vals0\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals/mapflow/_get_L1vals0\".\n",
      "190513-14:57:20,711 workflow INFO:\n",
      "\t [Node] Running \"_get_L1vals0\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190513-14:57:22,597 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 3 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals0\n",
      "                       * _get_peaks3\n",
      "190513-14:58:54,802 workflow INFO:\n",
      "\t [Node] Finished \"_get_peaks3\".\n",
      "190513-14:58:56,697 workflow INFO:\n",
      "\t [Job 15] Completed (_get_peaks3).\n",
      "190513-14:58:56,703 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 4 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals0\n",
      "190513-14:58:56,793 workflow INFO:\n",
      "\t [Node] Setting-up \"grouplevel_sescomposite.get_peaks\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks\".\n",
      "190513-14:58:56,802 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_peaks0\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks/mapflow/_get_peaks0\".\n",
      "190513-14:58:56,806 workflow INFO:\n",
      "\t [Node] Cached \"_get_peaks0\" - collecting precomputed outputs\n",
      "190513-14:58:56,809 workflow INFO:\n",
      "\t [Node] \"_get_peaks0\" found cached.\n",
      "190513-14:58:56,813 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_peaks1\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks/mapflow/_get_peaks1\".\n",
      "190513-14:58:56,816 workflow INFO:\n",
      "\t [Node] Cached \"_get_peaks1\" - collecting precomputed outputs\n",
      "190513-14:58:56,818 workflow INFO:\n",
      "\t [Node] \"_get_peaks1\" found cached.\n",
      "190513-14:58:56,822 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_peaks2\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks/mapflow/_get_peaks2\".\n",
      "190513-14:58:56,824 workflow INFO:\n",
      "\t [Node] Cached \"_get_peaks2\" - collecting precomputed outputs\n",
      "190513-14:58:56,826 workflow INFO:\n",
      "\t [Node] \"_get_peaks2\" found cached.\n",
      "190513-14:58:56,829 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_peaks3\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_peaks/mapflow/_get_peaks3\".\n",
      "190513-14:58:56,832 workflow INFO:\n",
      "\t [Node] Cached \"_get_peaks3\" - collecting precomputed outputs\n",
      "190513-14:58:56,834 workflow INFO:\n",
      "\t [Node] \"_get_peaks3\" found cached.\n",
      "190513-14:58:56,839 workflow INFO:\n",
      "\t [Node] Finished \"grouplevel_sescomposite.get_peaks\".\n",
      "190513-14:58:58,699 workflow INFO:\n",
      "\t [Job 5] Completed (grouplevel_sescomposite.get_peaks).\n",
      "190513-14:58:58,704 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 3 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals0\n",
      "190513-14:58:58,781 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_L1vals1\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals/mapflow/_get_L1vals1\".\n",
      "190513-14:58:58,790 workflow INFO:\n",
      "\t [Node] Running \"_get_L1vals1\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190513-14:59:00,702 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 2 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals1\n",
      "                       * _get_L1vals0\n",
      "190513-15:01:22,525 workflow INFO:\n",
      "\t [Node] Finished \"_get_L1vals0\".\n",
      "190513-15:01:22,849 workflow INFO:\n",
      "\t [Job 16] Completed (_get_L1vals0).\n",
      "190513-15:01:22,853 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 2 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals1\n",
      "190513-15:01:22,933 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_L1vals2\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals/mapflow/_get_L1vals2\".\n",
      "190513-15:01:22,943 workflow INFO:\n",
      "\t [Node] Running \"_get_L1vals2\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190513-15:01:24,852 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 1 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals2\n",
      "                       * _get_L1vals1\n",
      "190513-15:03:41,911 workflow INFO:\n",
      "\t [Node] Finished \"_get_L1vals2\".\n",
      "190513-15:03:42,997 workflow INFO:\n",
      "\t [Job 18] Completed (_get_L1vals2).\n",
      "190513-15:03:43,1 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 1 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals1\n",
      "190513-15:03:43,87 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_L1vals3\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals/mapflow/_get_L1vals3\".\n",
      "190513-15:03:43,97 workflow INFO:\n",
      "\t [Node] Running \"_get_L1vals3\" (\"nipype.interfaces.utility.wrappers.Function\")\n",
      "190513-15:03:44,998 workflow INFO:\n",
      "\t [MultiProc] Running 2 tasks, and 0 jobs ready. Free memory (GB): 56.15/56.55, Free processors: 0/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals3\n",
      "                       * _get_L1vals1\n",
      "190513-15:06:28,901 workflow INFO:\n",
      "\t [Node] Finished \"_get_L1vals1\".\n",
      "190513-15:06:29,171 workflow INFO:\n",
      "\t [Job 17] Completed (_get_L1vals1).\n",
      "190513-15:06:29,176 workflow INFO:\n",
      "\t [MultiProc] Running 1 tasks, and 0 jobs ready. Free memory (GB): 56.35/56.55, Free processors: 1/2.\n",
      "                     Currently running:\n",
      "                       * _get_L1vals3\n",
      "190513-15:30:51,167 workflow INFO:\n",
      "\t [Node] Finished \"_get_L1vals3\".\n",
      "190513-15:30:52,662 workflow INFO:\n",
      "\t [Job 19] Completed (_get_L1vals3).\n",
      "190513-15:30:52,666 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-15:30:52,753 workflow INFO:\n",
      "\t [Node] Setting-up \"grouplevel_sescomposite.get_L1vals\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals\".\n",
      "190513-15:30:52,764 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_L1vals0\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals/mapflow/_get_L1vals0\".\n",
      "190513-15:30:52,769 workflow INFO:\n",
      "\t [Node] Cached \"_get_L1vals0\" - collecting precomputed outputs\n",
      "190513-15:30:52,771 workflow INFO:\n",
      "\t [Node] \"_get_L1vals0\" found cached.\n",
      "190513-15:30:52,775 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_L1vals1\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals/mapflow/_get_L1vals1\".\n",
      "190513-15:30:52,779 workflow INFO:\n",
      "\t [Node] Cached \"_get_L1vals1\" - collecting precomputed outputs\n",
      "190513-15:30:52,781 workflow INFO:\n",
      "\t [Node] \"_get_L1vals1\" found cached.\n",
      "190513-15:30:52,787 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_L1vals2\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals/mapflow/_get_L1vals2\".\n",
      "190513-15:30:52,794 workflow INFO:\n",
      "\t [Node] Cached \"_get_L1vals2\" - collecting precomputed outputs\n",
      "190513-15:30:52,797 workflow INFO:\n",
      "\t [Node] \"_get_L1vals2\" found cached.\n",
      "190513-15:30:52,802 workflow INFO:\n",
      "\t [Node] Setting-up \"_get_L1vals3\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/get_L1vals/mapflow/_get_L1vals3\".\n",
      "190513-15:30:52,806 workflow INFO:\n",
      "\t [Node] Cached \"_get_L1vals3\" - collecting precomputed outputs\n",
      "190513-15:30:52,808 workflow INFO:\n",
      "\t [Node] \"_get_L1vals3\" found cached.\n",
      "190513-15:30:52,816 workflow INFO:\n",
      "\t [Node] Finished \"grouplevel_sescomposite.get_L1vals\".\n",
      "190513-15:30:54,664 workflow INFO:\n",
      "\t [Job 6] Completed (grouplevel_sescomposite.get_L1vals).\n",
      "190513-15:30:54,669 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 1 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n",
      "190513-15:30:54,759 workflow INFO:\n",
      "\t [Node] Setting-up \"grouplevel_sescomposite.datasink\" in \"/home/camachocm2/Analysis/aggregate_anats/workflows/grouplevel_sescomposite/datasink\".\n",
      "190513-15:30:54,770 workflow INFO:\n",
      "\t [Node] Running \"datasink\" (\"nipype.interfaces.io.DataSink\")\n",
      "190513-15:30:54,787 workflow INFO:\n",
      "\t [Node] Finished \"grouplevel_sescomposite.datasink\".\n",
      "190513-15:30:56,666 workflow INFO:\n",
      "\t [Job 7] Completed (grouplevel_sescomposite.datasink).\n",
      "190513-15:30:56,671 workflow INFO:\n",
      "\t [MultiProc] Running 0 tasks, and 0 jobs ready. Free memory (GB): 56.55/56.55, Free processors: 2/2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7f5fd88db198>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analysis workflow\n",
    "\n",
    "grouplevel = Workflow(name='grouplevel_sescomposite')\n",
    "\n",
    "grouplevel.connect([(grabdata, select_usable_data,[('data_list', 'data_list')]),\n",
    "                    (select_usable_data, model, [('final_data','in_file')]),\n",
    "                    (select_usable_data, model, [('design_file','design_mat')]),\n",
    "                    (select_usable_data, calc_threshold, [('final_subjects_list','subject_data_list')]),\n",
    "                    (select_usable_data, get_L1vals, [('final_subjects_list','subject_ids')]),\n",
    "                    (calc_threshold, cluster, [('tstat_threshold','threshold')]),\n",
    "                    \n",
    "                    (model, cluster, [('tstat_files','in_file')]),\n",
    "                    (cluster, get_peaks, [('index_file','clusters_file')]),\n",
    "                    (model, get_peaks, [('tstat_files','stat_file')]),\n",
    "                    (cluster, get_L1vals ,[('index_file','cluster_index_file')]),\n",
    "                    (select_usable_data, get_L1vals, [('final_data','sample_L1_data')]),\n",
    "                    \n",
    "                    (get_peaks, datasink, [('cluster_info_file','cluster_stats')]),\n",
    "                    (get_L1vals, datasink, [('extracted_L1vals_csv','cluster_L1vals')]),\n",
    "                    (model, datasink, [('tstat_files','tstat_files')]),\n",
    "                    (cluster, datasink, [('index_file','clusters_file')])\n",
    "                   ])\n",
    "\n",
    "grouplevel.base_dir = workflow_dir\n",
    "grouplevel.write_graph(graph2use='flat')\n",
    "grouplevel.run('MultiProc', plugin_args={'n_procs': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Effects Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "from os.path import join\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "from nipype.interfaces.utility import IdentityInterface, Function\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.interfaces.fsl.utils import Merge, ImageMeants, Split\n",
    "from nipype.interfaces.fsl.model import Randomise, Cluster\n",
    "from nipype.interfaces.freesurfer.model import Binarize\n",
    "from nipype.interfaces.fsl.maths import ApplyMask, Threshold\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "# MATLAB setup - Specify path to current SPM and the MATLAB's default mode\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('~/spm12/toolbox')\n",
    "MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "\n",
    "# FSL set up- change default file output type\n",
    "from nipype.interfaces.fsl import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI')\n",
    "\n",
    "# Set study variables\n",
    "analysis_home = '/Users/catcamacho/Box/LNCD_rewards_connectivity'\n",
    "firstlevel_dir = analysis_home + '/proc/firstlevel'\n",
    "secondlevel_dir = analysis_home + '/proc/secondlevel'\n",
    "workflow_dir = analysis_home + '/workflows'\n",
    "template_dir = analysis_home + '/templates'\n",
    "MNI_template = template_dir + '/MNI152_T1_1mm_brain.nii.gz'\n",
    "MNI_mask = template_dir + '/MNI152_T1_3mm_mask.nii.gz'\n",
    "\n",
    "#pull subject info \n",
    "subject_info = analysis_home + '/misc/subjs.csv'\n",
    "\n",
    "conditions = ['punish','neutral']\n",
    "seed_names = ['L_amyg','R_amyg']\n",
    "\n",
    "# Group analysis models (predicting FC)\n",
    "models = ['brain ~ ageMC + sex + ageMC*sex', \n",
    "          'brain ~ invAgeMC + sex + invAgeMC*sex']\n",
    "\n",
    "model_names = ['linearAge', 'inverseAge']\n",
    "\n",
    "terms = ['age', 'sex', 'ageSexInteract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMEM for MRI data (3D nifti data)\n",
    "def mri_lmem(model, mask, subject_dataframe, subject_files, grouping_variable):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "\n",
    "    from os import getcwd\n",
    "    from os.path import abspath\n",
    "    import statsmodels.formula.api as smf\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from numpy import array, empty_like, stack, nditer, zeros_like, zeros\n",
    "    from pandas import DataFrame, read_csv, Series, concat\n",
    "    from warnings import filterwarnings\n",
    "    filterwarnings(\"ignore\")\n",
    "\n",
    "    working_dir = getcwd() + '/'\n",
    "    subj_data = read_csv(subject_dataframe, header=0, index_col=0)\n",
    "\n",
    "    # Load the brain data\n",
    "    brain_niftis = load(subject_files)\n",
    "    brain_data_4D = brain_niftis.get_data()\n",
    "\n",
    "    # Load the mask\n",
    "    mask_nifti = load(mask)\n",
    "    mask = mask_nifti.get_data()\n",
    "\n",
    "    ## Preallocate the output arrays\n",
    "    # for the model\n",
    "    BIC_data = zeros_like(mask).astype(float)\n",
    "    AIC_data = zeros_like(mask).astype(float)\n",
    "    pval_intercept_data = zeros_like(mask).astype(float)\n",
    "    pval_age_data = zeros_like(mask).astype(float)\n",
    "    pval_sex_data = zeros_like(mask).astype(float)\n",
    "    pval_ageSexInteract_data = zeros_like(mask).astype(float)\n",
    "    # per subject\n",
    "    residuals_data = zeros_like(brain_data_4D).astype(float)\n",
    "    pred_values_data = zeros_like(brain_data_4D).astype(float)\n",
    "\n",
    "    # Set up the actual loops to pull in subject data and do the modeling\n",
    "    for x in range(0,mask.shape[0]):\n",
    "        for y in range(0,mask.shape[1]):\n",
    "            for z in range(0,mask.shape[2]):\n",
    "                if mask[x][y][z] == 1:\n",
    "                    voxel = zeros(brain_data_4D.shape[3])\n",
    "                    for a in range(0,brain_data_4D.shape[3]):\n",
    "                        voxel[a] = brain_data_4D[x][y][z][a]\n",
    "                    voxel = Series(voxel, index=subj_data.index, name='brain')\n",
    "                    data = concat([voxel, subj_data],axis=1)\n",
    "                    mlm = smf.mixedlm(model, data, groups=data[grouping_variable])\n",
    "                    mod = mlm.fit()\n",
    "                    pval_intercept_data[x][y][z] = 1 - mod.pvalues[0]\n",
    "                    pval_age_data[x][y][z] = 1 - mod.pvalues[1]\n",
    "                    pval_sex_data[x][y][z] = 1 - mod.pvalues[2]\n",
    "                    pval_ageSexInteract_data[x][y][z] = 1 - mod.pvalues[3]\n",
    "                    BIC_data[x][y][z] = mod.bic\n",
    "                    AIC_data[x][y][z] = mod.aic\n",
    "                    residuals = mod.resid\n",
    "                    pred_values = Series(mod.predict(), index = subj_data.index)\n",
    "                    for d in range(0,brain_data_4D.shape[3]):\n",
    "                        residuals_data[x][y][z][d] = residuals.tolist()[d]\n",
    "                        pred_values_data[x][y][z][d] = pred_values.tolist()[d]\n",
    "\n",
    "                \n",
    "    # Save the ouputs as nifti files\n",
    "    output_data = [BIC_data, AIC_data, pval_intercept_data, pval_age_data,\n",
    "                    pval_sex_data, pval_ageSexInteract_data, residuals_data, \n",
    "                    pred_values_data]\n",
    "    output_niftis = [Nifti1Image(result, mask_nifti.affine) for result in output_data]\n",
    "    \n",
    "    output_filenames = ['BICs.nii','AICs.nii','pval_intercept_data.nii',\n",
    "                        'pval_age_data.nii','pval_sex_data.nii',\n",
    "                        'pval_ageSexInteract_data.nii','residuals_data.nii',\n",
    "                        'pred_values_data.nii']\n",
    "    for e in range(0,len(output_niftis)):\n",
    "        save(output_niftis[e], working_dir + output_filenames[e])\n",
    "    \n",
    "    output_volumes = [abspath(output_filenames[0]),\n",
    "                      abspath(output_filenames[1]),\n",
    "                      abspath(output_filenames[2]), \n",
    "                      abspath(output_filenames[3]), \n",
    "                      abspath(output_filenames[4]), \n",
    "                      abspath(output_filenames[5]), \n",
    "                      abspath(output_filenames[6]), \n",
    "                      abspath(output_filenames[7])]\n",
    "    \n",
    "    return(output_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling nodes\n",
    "\n",
    "conditionsource = Node(IdentityInterface(fields=['condition','seed']),\n",
    "                       name='conditionsource')\n",
    "conditionsource.iterables = [('condition',conditions),('seed', seed_names)]\n",
    "\n",
    "# Grab the subject beta maps \n",
    "time_template = {'beta_maps':firstlevel_dir + '/smoothedMNI_conn_beta/*/%s/%s/betas_flirt_smooth_masked.nii'}\n",
    "betamap_grabber = Node(DataGrabber(sort_filelist=True,\n",
    "                                   field_template = time_template,\n",
    "                                   base_directory=firstlevel_dir,\n",
    "                                   template=firstlevel_dir + '/smoothedMNI_conn_beta/*/%s/%s/betas_flirt_smooth_masked.nii',\n",
    "                                   infields=['condition','seed'],\n",
    "                                   template_args={'beta_maps':[['condition','seed']]}), \n",
    "                       name='betamap_grabber')\n",
    "\n",
    "# Sink relavent data\n",
    "substitutions = [('_condition_',''),\n",
    "                 ('_seed_',''), \n",
    "                 ('brain~ageMC+sex+ageMC*sex','linearAge'),\n",
    "                 ('brain~invAgeMC+sex+invAgeMC*sex','inverseAge')]\n",
    "datasink = Node(DataSink(substitutions=substitutions, \n",
    "                         base_directory=secondlevel_dir,\n",
    "                         container=secondlevel_dir), \n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis nodes\n",
    "\n",
    "#Merge subject files together\n",
    "merge = Node(Merge(dimension='t'), name='merge')\n",
    "\n",
    "# Linear mixed effects modeling\n",
    "lmemodel = Node(Function(input_names = ['model', 'mask', 'subject_dataframe', \n",
    "                                        'subject_files', 'grouping_variable'], \n",
    "                         output_names = ['output_volumes'], \n",
    "                         function=mri_lmem), \n",
    "                name='lmemodel')\n",
    "lmemodel.iterables = [('model', models)]\n",
    "lmemodel.inputs.mask = MNI_mask\n",
    "lmemodel.inputs.subject_dataframe = subject_info\n",
    "lmemodel.inputs.grouping_variable = 'Timepoint'\n",
    "\n",
    "# Mask the file to only significant voxels for clustering\n",
    "mask_stat = Node(Binarize(), name = 'mask_stat')\n",
    "\n",
    "# Cluster the results\n",
    "cluster_results = MapNode(Cluster(threshold=0.95,\n",
    "                                  out_index_file=True,\n",
    "                                  out_localmax_txt_file=True),\n",
    "                          name='cluster_results', \n",
    "                          iterfield = ['in_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LMEManalysisflow = Workflow(name='LMEManalysisflow')\n",
    "LMEManalysisflow.connect([(conditionsource, betamap_grabber, [('condition','condition'),\n",
    "                                                              ('seed','seed')]),\n",
    "                          (betamap_grabber, merge, [('beta_maps','in_files')]),\n",
    "                          (merge, lmemodel, [('merged_file','subject_files')]),\n",
    "                          (merge, datasink, [('merged_file','merged_subj_betas')]),\n",
    "                          (lmemodel, datasink, [('output_volumes','output_volumes')])\n",
    "                         ])\n",
    "#LMEManalysisflow.base_dir = workflow_dir\n",
    "#LMEManalysisflow.write_graph(graph2use='flat')\n",
    "#LMEManalysisflow.run('MultiProc', plugin_args={'n_procs':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_clusters(cluster_index):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "\n",
    "    from os import getcwd\n",
    "    from os.path import abspath\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from numpy import unique, amax, stack, zeros_like\n",
    "\n",
    "    cluster_min = 20\n",
    "    nifti = load(cluster_index)\n",
    "    data = nifti.get_data()\n",
    "    clust_labels, vox_count = unique(data, return_counts=True)\n",
    "    clust_labels = clust_labels.tolist()\n",
    "    vox_count = vox_count.tolist()\n",
    "\n",
    "    iter = 0\n",
    "    for i in range(0, len(clust_labels)):\n",
    "        if vox_count[i] < cluster_min:\n",
    "            del(clust_labels[i-iter])\n",
    "            iter = iter +1\n",
    "\n",
    "    clust_labels = clust_labels[1:]\n",
    "    num_clusters = len(clust_labels)\n",
    "\n",
    "    cluster_masks = []\n",
    "    for a in range(0,num_clusters):\n",
    "        temp = zeros_like(data)\n",
    "        temp[data==clust_labels[a]] = 1\n",
    "        cluster_masks.append(temp)\n",
    "\n",
    "    cluster_masks_4D = stack(cluster_masks,axis=3)\n",
    "    cluster_masks_nifti = Nifti1Image(cluster_masks_4D, nifti.affine)\n",
    "    save(cluster_masks_nifti, 'new_clust_index.nii')\n",
    "    newcluster_index = abspath('new_clust_index.nii')\n",
    "    return(newcluster_index)\n",
    "\n",
    "\n",
    "def finalize_models(cluster, template, betas_text_file, subj_data, model_name):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import abspath\n",
    "    import statsmodels.formula.api as smf\n",
    "    from pandas import DataFrame, Series, concat, read_table, read_csv\n",
    "    from ggplot import *\n",
    "    from warnings import filterwarnings\n",
    "    filterwarnings(\"ignore\")\n",
    "    import sys\n",
    "    from nilearn import plotting\n",
    "    \n",
    "    #determine which model to use\n",
    "    if model_name=='linearAge':\n",
    "        model = 'brain ~ ageMC + sex + ageMC*sex'\n",
    "    elif model_name=='inverseAge':\n",
    "        model = 'brain ~ invAgeMC + sex + invAgeMC*sex'\n",
    "        \n",
    "    origstdout = sys.stdout\n",
    "    sys.stdout = open('modelsummary.txt', 'w')\n",
    "\n",
    "    #organize data into dataframe for modeling\n",
    "    subj_data = read_csv(subj_data, header=0, index_col=0)\n",
    "    brain_data = read_table(betas_text_file, header=None, names='brain', index_col=None)\n",
    "    data = concat([brain_data, subj_data],axis=1)\n",
    "    # do the modeling\n",
    "    mlm = smf.mixedlm(model, data, groups=data[grouping_variable])\n",
    "    mod = mlm.fit()\n",
    "    print(mod.summary())\n",
    "    sys.stdout = origstdout\n",
    "    close('modelsummary.txt')\n",
    "    summary_file = abspath('modelsummary.txt')\n",
    "    \n",
    "    # plot the model results\n",
    "    figure = ggplot(data, aes(x='age',y='brain') + theme_classic() + \n",
    "                    geom_point() + geom_smooth(method=lm,se=True, size=2))\n",
    "    figure.save('plot.svg')\n",
    "    figure_file = abspath('plot.svg')\n",
    "    \n",
    "    # make a picture of the brain cluster\n",
    "    display = plotting.plot_anat(anat_img = template, display_mode='x')\n",
    "    display.add_overlay(cluster, plotting.cm.purple_green, threshold=1)\n",
    "    display.savefig('clusterpic.png')\n",
    "    display.close()\n",
    "    clusterpic_file = abspath('clusterpic.png')\n",
    "    \n",
    "    outputs = [summary_file, figure_file, clusterpic_file]\n",
    "    \n",
    "    return(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data outputs from the LMEMs\n",
    "infosource = Node(IdentityInterface(fields=['condition','seed','term','model']),\n",
    "                  name='conditionsource')\n",
    "infosource.iterables = [('condition', conditions), \n",
    "                        ('seed', seed_names), \n",
    "                        ('term', terms), \n",
    "                        ('model', model_names)]\n",
    "\n",
    "lme_template = {'pval_vol': secondlevel_dir + '/output_volumes/{condition}{seed}/_model_{model}/pval_{term}_data.nii', \n",
    "                'subj_beta_data': secondlevel_dir + '/merged_betas/betas_merged.nii'}            \n",
    "lme_datagrabber = Node(SelectFiles(lme_template), name='lme_datagrabber') \n",
    "\n",
    "# Threshold out nonsignificant voxels\n",
    "threshold = Node(Threshold(thresh=0.95), name='threshold')\n",
    "\n",
    "# Cluster the remaining volumes\n",
    "cluster = Node(Cluster(threshold=0.95, \n",
    "                       out_index_file=True, \n",
    "                       use_mm=True,\n",
    "                       minclustersize=True,\n",
    "                       peak_distance=6), \n",
    "               name='cluster')\n",
    "\n",
    "# remove small clusters\n",
    "cluster_min = Node(Function(input_names=['cluster_index'], \n",
    "                            output_names=['newcluster_index'], \n",
    "                            function=threshold_clusters), \n",
    "                   name='cluster_min')\n",
    "\n",
    "# Split the clusters\n",
    "split = Node(Split(dimension='t'),\n",
    "             name='split')\n",
    "\n",
    "# Extract mean connectivity per cluster\n",
    "pull_mean_betas = MapNode(ImageMeants(out_file='mean_connectivity.txt'), \n",
    "                          name= 'pull_mean_betas', \n",
    "                          iterfield=['mask'])\n",
    "                         \n",
    "\n",
    "# graph the connectivity against age and re-do the linear models\n",
    "finalize_models = MapNode(Function(input_names=['cluster','template','betas_text_file','subj_data', 'model_name'], \n",
    "                                   output_names=['outputs'], \n",
    "                                   function=finalize_models),\n",
    "                          name='finalize_models', \n",
    "                          iterfield=['betas_text_file'])\n",
    "finalize_models.inputs.subj_data=subject_info\n",
    "finalize_models.inputs.template=MNI_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusterflow = Workflow(name='clusterflow')\n",
    "clusterflow.connect([(infosource, lme_datagrabber, [('condition','condition'),\n",
    "                                                    ('seed','seed'),\n",
    "                                                    ('term','term'),\n",
    "                                                    ('model','model')]),\n",
    "                     (lme_datagrabber,threshold, [('pval_vol','in_file')]),\n",
    "                     (threshold, cluster, [('out_file','in_file')]),\n",
    "                     (cluster, cluster_min, [('index_file','cluster_index')]),\n",
    "                     (cluster_min, split, [('newcluster_index','in_file')]),\n",
    "                     (split, pull_mean_betas, [('out_files','mask')]),\n",
    "                     (lme_datagrabber, pull_mean_betas, [('subj_beta_data','in_file')]),\n",
    "                     \n",
    "                     (pull_mean_betas, datasink, [('out_file','beta_values')]),\n",
    "                     (cluster, datasink, [('index_file','cluster_index_file'),\n",
    "                                          ('localmax_txt_file','cluster_localmax_txt_file')]),\n",
    "                     (split,datasink,[('out_files','final_clusters')])\n",
    "                    ])\n",
    "clusterflow.base_dir = workflow_dir\n",
    "clusterflow.write_graph(graph2use='flat')\n",
    "clusterflow.run('MultiProc', plugin_args={'n_procs':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cluster_betas(cluster_file, sample_L1_data, clust_name, cluster_vals):\n",
    "    from pandas import DataFrame, Series, read_csv\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    \n",
    "    sample_data = read_csv(cluster_vals, index_col=0)\n",
    "\n",
    "    out_prefix = clust_name\n",
    "    \n",
    "    eb = ImageMeants()\n",
    "    eb.inputs.in_file = sample_L1_data\n",
    "    eb.inputs.mask = cluster_file\n",
    "    eb.inputs.out_file = 'L1vals.txt'\n",
    "    eb.run()\n",
    "    L1vals = open('L1vals.txt').read().splitlines()\n",
    "    sample_data[clust_name] = Series(L1vals, index=sample_data.index)\n",
    "    sample_data.to_csv(cluster_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output='/Users/catcamacho/Box/Conferences/20190321_SRCD/talk/1factor/'\n",
    "cluster_vals=output+'clusters_extracted_L1vals.csv'\n",
    "clust_files=[output+'LIPS_ClusterMask_randomise_tstat1_x=4.252129e+01_y=-2.092878e+01_z=3.774994e+01_59voxels.nii', \n",
    "             output+'LITG_ClusterMask_randomise_tstat1_x=4.952129e+01_y=7.123242e-02_z=-2.325006e+01_190voxels.nii', \n",
    "             output+'PSTS_ClusterMask_randomise_tstat1_x=5.052129e+01_y=-2.392877e+01_z=1.274994e+01_72voxels.nii', \n",
    "             output+'RIFG_ClusterMask_randomise_tstat1_x=-5.047871e+01_y=1.207123e+01_z=1.774995e+01_74voxels.nii', \n",
    "             output+'RITG_ClusterMask_randomise_tstat1_x=-2.447871e+01_y=-1.792877e+01_z=-1.425006e+01_109voxels.nii']\n",
    "clust_names=['LIPS','LITG','PSTS','RIFG','RITG']\n",
    "sample_data = output + 'data_merged_smooth.nii.gz'\n",
    "\n",
    "for a in range(0,5):\n",
    "    extract_cluster_betas(clust_files[a], sample_data, clust_names[a], cluster_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "thick_data = read_csv(cluster_vals, index_col=0)\n",
    "full_data = read_csv(output+'all_data_20190320.csv', index_col=0)\n",
    "full_data = full_data.merge(thick_data, on='freesurferID',how='outer')\n",
    "#other_data = read_csv(output+'combined_data_20190311.csv', index_col=0)\n",
    "#vol_data = read_csv(output+'residual_complete_data.csv', index_col=0)\n",
    "#full_data = thick_data.merge(other_data, on='freesurferID',how='outer')\n",
    "#full_data = full_data.merge(vol_data, on='freesurferID',how='outer')\n",
    "full_data.to_csv(output+'all_data_20190320.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data=read_csv(output+'all_data_20190320.csv',index_col=0)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.formula.api import ols\n",
    "screg = ['res_putamen', 'res_pall']\n",
    "region=['LIPS', 'LITG', 'PSTS', 'RIFG', 'RITG']\n",
    "#for a in range(0,2):\n",
    "#    for b in range(0,4):\n",
    "#        print(screg[a]+ ' '+region[b]+' '+str(spearmanr(data[screg[a]],data[region[b]],nan_policy='omit')))\n",
    "#        model = ols('CBCL_externalizing ~ '+region[b]+' + '+region[b]+'*Factor1 + Age_yrs + eTIV + male', \n",
    "#data).fit()\n",
    "#        print(model.summary())\n",
    "        \n",
    "#print(spearmanr(data['PSTS'],data['CBCL_externalizing'],nan_policy='omit'))\n",
    "#print(spearmanr(data['PSTS'],data['Factor1'],nan_policy='omit'))\n",
    "print(spearmanr(data['CBCL_externalizing'],data['res_putamen'],nan_policy='omit'))\n",
    "print(spearmanr(data['CBCL_externalizing'],data['res_pall'],nan_policy='omit'))\n",
    "\n",
    "\n",
    "model = ols('CBCL_externalizing ~ PSTS + PSTS*Factor1 + Age_yrs + eTIV + male', data).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='talk',style='white')\n",
    "sns.jointplot('LIPS','Factor1',data=data, kind='reg')\n",
    "plt.savefig(output+'IPL_f1_correlation.svg')\n",
    "from scipy.stats import spearmanr\n",
    "#region=['LIPS', 'LITG', 'PSTS', 'RIFG', 'RITG']\n",
    "#for a in region:\n",
    "#    print(spearmanr(data[a],data['CBCL_externalizing'],nan_policy='omit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_assign(c):\n",
    "    if c['Factor1']<=0:\n",
    "        return 'low'\n",
    "    elif c['Factor1']>0:\n",
    "        return 'high'\n",
    "sns.set(context='talk', style='white')\n",
    "\n",
    "data['irr_group'] = data.apply(group_assign, axis=1) \n",
    "sns.lmplot(x='PSTS',y='CBCL_externalizing',hue='irr_group', data=data, ci=None)\n",
    "plt.savefig(output+'irr_PSTS_interact.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "males= data[data['male']==1]\n",
    "females=data[data['male']==0]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist([males['Age_yrs'],females['Age_yrs']], bins=10, stacked=True, \n",
    "         label=['Male','Female'])\n",
    "plt.legend()\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.savefig(output + 'age_sex_hist.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = data[data['seq1']==1]\n",
    "seq2 = data[data['seq2']==1]\n",
    "seq3 = data[data['seq3']==1]\n",
    "seq4 = data[data['seq4']==1]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist([seq1['Age_yrs'],seq2['Age_yrs'],seq3['Age_yrs'],seq4['Age_yrs']],bins=10,\n",
    "         stacked=True, label=['version 1', 'version 2','version 3','version 4'])\n",
    "plt.legend()\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.savefig(output + 'age_sequence_hist.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
