{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Gray Matter Markers of Irritability: a machine learning approach\n",
    "This notebook is designed to analyze previously processed gray matter density volumes using support vector regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "from nipype.interfaces.utility import IdentityInterface, Function\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.interfaces.spm.preprocess import VBMSegment, Segment\n",
    "from nipype.interfaces.ants import Atropos, Registration, ApplyTransforms, N4BiasFieldCorrection\n",
    "from nipype.interfaces.fsl import ApplyMask, BET\n",
    "from pandas import DataFrame, Series, read_csv\n",
    "\n",
    "# Study specific variables\n",
    "study_home = '/moochie/user_data/CamachoCat/Aggregate_anats/GMD_ML'\n",
    "\n",
    "sub_data_file = study_home + '/doc/subject_info.csv'\n",
    "subject_info = read_csv(sub_data_file, index_col=0)\n",
    "subjects_list = subject_info['freesurferID'].tolist()\n",
    "\n",
    "preproc_dir = study_home + '/proc'\n",
    "output_dir = study_home + '/ml_trainingset'\n",
    "\n",
    "sample_template = study_home + '/templates/lcbd_template_1mm.nii.gz'\n",
    "sample_template_brain = study_home + '/templates/lcbd_template_1mm_brain.nii.gz'\n",
    "sample_template_mask = study_home + '/templates/lcbd_template_1mm_mask.nii.gz'\n",
    "\n",
    "subject_info.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(context='paper',style='white')\n",
    "#for variable in ['Age_yrs', 'MAP_Temper_Loss','MAP_Noncompliance','MAP_General_Aggression','MAP_Low_Concern']:\n",
    "#    plt.figure()\n",
    "#    sns.distplot(subject_info[variable],hist=True,kde=False,bins=30, color='#171C43', hist_kws={'edgecolor':'black'})\n",
    "    #plt.savefig(variable+'_hist.svg')\n",
    "    \n",
    "for variable in ['MAP_Temper_Loss','MAP_Noncompliance','MAP_General_Aggression','MAP_Low_Concern']:\n",
    "    plt.figure()\n",
    "    a = sns.jointplot(subject_info['Age_yrs'],subject_info[variable],\n",
    "                      marginal_kws={'kde':False,'bins':30})\n",
    "    a.annotate(stats.pearsonr,fontsize=12)\n",
    "    #plt.savefig(variable+'_age_corr.svg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from numpy import squeeze\n",
    "\n",
    "## Create a conditions list for the feature set\n",
    "age_labels = subject_info[['Age_yrs']].copy()\n",
    "age_labels = age_labels.values\n",
    "irr_labels = subject_info[['MAP_Temper_Loss','MAP_Noncompliance','MAP_General_Aggression','MAP_Low_Concern']].copy()\n",
    "irr_labels = irr_labels.values\n",
    "\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "scaler.fit(age_labels)\n",
    "sd_agedata = scaler.transform(age_labels)\n",
    "\n",
    "pt = PowerTransformer()\n",
    "pt.fit(irr_labels)\n",
    "pt_irritability = pt.transform(irr_labels)\n",
    "pt_irritability = squeeze(pt_irritability)\n",
    "\n",
    "subject_info = subject_info.merge(DataFrame(pt_irritability,\n",
    "                                            columns=['temploss_yj','noncomp_yj','genagg_yj','lowcon_yj'],\n",
    "                                            index=subject_info.index),left_index=True, right_index=True)\n",
    "subject_info['age_cent'] = sd_agedata\n",
    "\n",
    "#subject_info.to_csv(output_dir + '/featureset_key.csv')\n",
    "subject_info.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.scatterplot(x='MAP_Temper_Loss',y='temploss_yj',data=subject_info)\n",
    "plt.figure()\n",
    "plt.hist(subject_info['temploss_yj'][np.isfinite(subject_info['temploss_yj'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate all the parameter estimates from preproc to create a feature set\n",
    "from nipype.interfaces.fsl.utils import Merge\n",
    "\n",
    "gm_template = preproc_dir + '/final_gmd/{0}/final_smooth_gm.nii.gz'\n",
    "gm_files = []\n",
    "for sub in subjects_list:\n",
    "    gm_files.append(gm_template.format(sub))\n",
    "gmd_feature_data = output_dir + '/gmd_combined.nii.gz'\n",
    "#print(gm_files)\n",
    "merge = Merge()\n",
    "merge.inputs.in_files = gm_files\n",
    "merge.inputs.dimension = 't'\n",
    "merge.inputs.merged_file = gmd_feature_data\n",
    "#merge.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenate all the parameter estimates from preproc to create a feature set\n",
    "from glob import glob\n",
    "from nipype.interfaces.fsl.utils import Merge\n",
    "from nipype.interfaces.fsl import SUSAN, Threshold\n",
    "files = glob(preproc_dir + '/soft_tissue_files/*/POSTERIOR_02.nii.gz')\n",
    "files = sorted(files)\n",
    "\n",
    "def brightthresh(img):\n",
    "    import nibabel as nib\n",
    "    from numpy import median, where\n",
    "    \n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    \n",
    "    img_nifti1 = nib.load(img)\n",
    "    img_data = img_nifti1.get_data()\n",
    "    img_data = img_data.astype(float)\n",
    "    \n",
    "    brain_values = where(img_data > 0)\n",
    "    median_thresh = median(brain_values)\n",
    "    bright_thresh = 0.75 * median_thresh\n",
    "    \n",
    "    return(bright_thresh)\n",
    "\n",
    "sm = SUSAN()\n",
    "sm.inputs.fwhm=4\n",
    "thr = Threshold()\n",
    "thr.inputs.thresh=0.05\n",
    "thr.inputs.direction='below'\n",
    "\n",
    "for file in files:\n",
    "    #sm.inputs.brightness_threshold = brightthresh(file)\n",
    "    sm.inputs.in_file = file\n",
    "    sm.inputs.out_file = file.replace('POSTERIOR_02','smoothed_gm')\n",
    "    #sm.run()\n",
    "    thr.inputs.in_file = sm.inputs.out_file\n",
    "    thr.inputs.out_file = file.replace('POSTERIOR_02','final_smooth_gm')\n",
    "    #thr.run()\n",
    "    \n",
    "\n",
    "gm_files = glob(preproc_dir + '/soft_tissue_files/*/final_smooth_gm.nii.gz')\n",
    "gm_files = sorted(gm_files)\n",
    "gmd_feature_data = output_dir + '/gmd_combined.nii.gz'\n",
    "\n",
    "merge = Merge()\n",
    "merge.inputs.in_files = gm_files\n",
    "merge.inputs.dimension = 't'\n",
    "merge.inputs.merged_file = gmd_feature_data\n",
    "#merge.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces.fsl import GLM, Merge\n",
    "from os.path import abspath\n",
    "from subprocess import check_call\n",
    "\n",
    "usable_subs = subject_info[subject_info['final_incl']==1]\n",
    "subjects_list = usable_subs['freesurferID'].tolist()\n",
    "ages = usable_subs['age_cent'].tolist()\n",
    "male = usable_subs['male'].tolist()\n",
    "seq1 = usable_subs['seq1'].tolist()\n",
    "seq2 = usable_subs['seq2'].tolist()\n",
    "seq3 = usable_subs['seq3'].tolist()\n",
    "seq4 = usable_subs['seq4'].tolist()\n",
    "eTIV = usable_subs['eTIV'].tolist()\n",
    "\n",
    "final_data_list = []\n",
    "text_file = open('temp_text.txt','w')\n",
    "\n",
    "for a in range(0,len(subjects_list)):\n",
    "    file = preproc_dir + '/final_gmd/{0}/final_smooth_gm.nii.gz'.format(subjects_list[a])\n",
    "    final_data_list.append(file)\n",
    "    text_file.write('{0} {1} {2} {3} {4} {5}\\n'.format(male[a], seq1[a], seq2[a], seq3[a],seq4[a], eTIV[a]))\n",
    "    \n",
    "text_file.close()\n",
    "\n",
    "file = abspath('temp_text.txt')\n",
    "check_call(['Text2Vest',file,'design.mat'])\n",
    "design_file = abspath('design.mat')\n",
    "\n",
    "me=Merge()\n",
    "me.inputs.dimension='t'\n",
    "me.inputs.in_files=final_data_list\n",
    "me.inputs.merged_file='data_merged.nii.gz'\n",
    "me.run()\n",
    "merged_gmd = abspath('data_merged.nii.gz')\n",
    "\n",
    "glm = GLM()\n",
    "glm.inputs.in_file = merged_gmd\n",
    "glm.inputs.design = design_file\n",
    "glm.inputs.mask = sample_template_mask\n",
    "glm.inputs.out_res_name = 'data_resids_noage.nii.gz'\n",
    "glm.run()\n",
    "\n",
    "final_data = abspath('data_resids_noage.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "analysis = 'factor4'\n",
    "masker = NiftiMasker(mask_img=sample_template_mask,standardize=True, \n",
    "                     memory='nilearn_cache', memory_level=1)\n",
    "X = masker.fit_transform(gmd_feature_data)\n",
    "\n",
    "if analysis == 'Age':\n",
    "    mask = subject_info['final_incl']==1\n",
    "    labels = subject_info['Age_yrs'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X=X[mask]\n",
    "elif analysis == 'Temper_Loss':\n",
    "    mask = (subject_info['MAP_Temper_Loss']>=0) & (subject_info['final_incl']==1)\n",
    "    labels = subject_info['temploss_yj'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X = X[mask]\n",
    "elif analysis == 'Noncompliance':\n",
    "    mask = (subject_info['MAP_Noncompliance']>=0) & (subject_info['final_incl']==1)\n",
    "    labels = subject_info['noncomp_yj'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X = X[mask]\n",
    "elif analysis == 'General_Aggression':\n",
    "    mask = (subject_info['MAP_General_Aggression']>=0) & (subject_info['final_incl']==1)\n",
    "    labels = subject_info['genagg_yj'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X = X[mask]\n",
    "elif analysis == 'Low_Concern':\n",
    "    mask = (subject_info['MAP_Low_Concern']>=0) & (subject_info['final_incl']==1)\n",
    "    labels = subject_info['lowcon_yj'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X = X[mask]\n",
    "elif analysis == 'factor1':\n",
    "    mask = (subject_info['smiling_laughter']>=0) & (subject_info['final_incl']==1)\n",
    "    labels = subject_info['factor1'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X = X[mask]\n",
    "elif analysis == 'factor2':\n",
    "    mask = (subject_info['smiling_laughter']>=0) & (subject_info['final_incl']==1)\n",
    "    labels = subject_info['factor2'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X = X[mask]\n",
    "elif analysis == 'factor3':\n",
    "    mask = (subject_info['smiling_laughter']>=0) & (subject_info['final_incl']==1)\n",
    "    labels = subject_info['factor3'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X = X[mask]\n",
    "elif analysis == 'factor4':\n",
    "    mask = (subject_info['smiling_laughter']>=0) & (subject_info['final_incl']==1)\n",
    "    labels = subject_info['factor4'][mask]\n",
    "    groups = subject_info['freesurferID'][mask]\n",
    "    X = X[mask]\n",
    "    \n",
    "analysis= analysis\n",
    "results_file = open(output_dir + '/results_' + analysis + '.txt','w')\n",
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the support vector classification\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn.feature_selection import f_regression, SelectPercentile\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "# Set up the regression\n",
    "svr = SVR(kernel='linear', C=1)\n",
    "\n",
    "feature_selection = SelectPercentile(f_regression, percentile=5)\n",
    "fs_svr = Pipeline([('feat_select', feature_selection), ('svr', svr)])\n",
    "\n",
    "# Run the regression\n",
    "fs_svr.fit(X, labels)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, LeaveOneGroupOut, RepeatedKFold\n",
    "\n",
    "cv = LeaveOneGroupOut()\n",
    "#cv = RepeatedKFold(n_splits=10,n_repeats=10)\n",
    "y_pred = cross_val_predict(fs_svr, X, y=labels, n_jobs=10,groups=groups,cv=cv)\n",
    "\n",
    "# save weights\n",
    "coef = svr.coef_\n",
    "coef = feature_selection.inverse_transform(coef)\n",
    "coef_image = masker.inverse_transform(coef)\n",
    "coef_image.to_filename(output_dir + '/svrweights_' + analysis + '.nii.gz')\n",
    "\n",
    "from scipy.stats import linregress\n",
    "slope, intercept, r_val, p_val, stderr = linregress(labels, y_pred) \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(labels, y_pred)\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "spear_r, spear_p = spearmanr(labels, y_pred)\n",
    "\n",
    "print(\"prediction accuracy: %.4f / p-value: %f / MSE: %f // Spearman: %f / p-value: %f\" % (r_val, p_val, mse, spear_r, spear_p))\n",
    "\n",
    "svr_results=DataFrame()\n",
    "svr_results['labels']=labels\n",
    "svr_results['y_pred']=Series(y_pred,index=labels.index)\n",
    "# plot the predicted versus actual values\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='poster',style='white')\n",
    "sns.lmplot(x='labels', y='y_pred',ci=None,data=svr_results)\n",
    "plt.xlabel('Actual ' + analysis)\n",
    "plt.ylabel('Predicted ' + analysis)\n",
    "plt.savefig(output_dir + '/scatter_pred_actual_' + analysis + '_poster.svg')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "results_file.write(\"Prediction accuracy r-value: %.4f / p-value: %f / MSE: %f // Spearman: %f / p-value: %f \\n\" % (r_val, p_val, mse, spear_r, spear_p))\n",
    "results_file.write('predicted: ' + str(y_pred) + '\\n')\n",
    "results_file.write('actual: ' + str(labels) + '\\n')\n",
    "\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import savetxt\n",
    "\n",
    "results_file = open(output_dir + '/perm_results_' + analysis + '.txt','w')\n",
    "\n",
    "score, permutation_scores, pvalue = permutation_test_score(fs_svr, X, labels, scoring='neg_mean_squared_error', \n",
    "                                                           cv=cv, n_permutations=500, n_jobs=20, groups=groups)\n",
    "savetxt(output_dir + '/permutation_scores_mse_' + analysis + '.txt', permutation_scores)\n",
    "\n",
    "# Save a figure of the permutation scores\n",
    "plt.hist(permutation_scores, 20, label='Permutation scores',\n",
    "         edgecolor='black')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "         label='Mean Squared Error (pvalue %f)' % pvalue)\n",
    "plt.ylim(ylim)\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.savefig(output_dir + '/permutation_plot_mse_' + analysis + '.svg', transparent=True)\n",
    "plt.close()\n",
    "\n",
    "# save final pval/classifier score\n",
    "results_file.write('MSE score %s (pvalue : %s) \\n' % (score, pvalue))\n",
    "\n",
    "## Perform permutation testing to get a p-value for r-squared\n",
    "score, permutation_scores, pvalue = permutation_test_score(fs_svr, X, labels, scoring='r2', \n",
    "                                                           cv=cv, n_permutations=500, n_jobs=20, groups=groups)\n",
    "savetxt(output_dir + '/permutation_scores_r2_' + analysis + '.txt', permutation_scores)\n",
    "\n",
    "# Save a figure of the permutation scores\n",
    "plt.hist(permutation_scores, 20, label='Permutation scores',\n",
    "         edgecolor='black')\n",
    "ylim = plt.ylim()\n",
    "plt.plot(2 * [score], ylim, '--g', linewidth=3,\n",
    "         label='R-squared (pvalue %f)' % pvalue)\n",
    "plt.ylim(ylim)\n",
    "plt.legend()\n",
    "plt.xlabel('Score')\n",
    "plt.savefig(output_dir + '/permutation_plot_r2_' + analysis + '.svg', transparent=True)\n",
    "plt.close()\n",
    "\n",
    "# save final pval/classifier score\n",
    "results_file.write('R square: %s (pvalue : %s) \\n' % (score, pvalue))\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "analysis = 'sequence_LOSO_6_10'\n",
    "masker = NiftiMasker(mask_img=sample_template_mask,standardize=True, \n",
    "                     memory='nilearn_cache', memory_level=1)\n",
    "X = masker.fit_transform(gmd_feature_data)\n",
    "mask=(conditions.age_yrs<=10) & (conditions.age_yrs>=6)\n",
    "\n",
    "if analysis == 'sequence_LOSO':\n",
    "    labels = conditions['sequence']\n",
    "    groups = conditions['subject'] \n",
    "elif analysis == 'sequence_LOGO':\n",
    "    labels = conditions['sequence']\n",
    "    groups = conditions['sequence'] \n",
    "elif analysis == 'sequence_LOSO_6_10':\n",
    "    labels = conditions['sequence'][mask]\n",
    "    groups = conditions['subject'][mask]\n",
    "    X=X[mask]\n",
    "\n",
    "    \n",
    "results_file = open(output_dir + '/results_' + analysis + '.txt','w')\n",
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the support vector classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import f_classif, SelectPercentile\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set up the support vector classifier\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Select the features contributing to the model\n",
    "feature_selection = SelectPercentile(f_classif, percentile=5) #0.05/228453 voxels\n",
    "fs_svc = Pipeline([('feat_select', feature_selection), ('svc', svc)])\n",
    "\n",
    "# Run the classifier\n",
    "fs_svc.fit(X, labels)\n",
    "\n",
    "# Obtain prediction values via cross validation\n",
    "from sklearn.model_selection import cross_validate, LeaveOneGroupOut, cross_val_predict\n",
    "\n",
    "loso = LeaveOneGroupOut()\n",
    "cv_scores = cross_validate(fs_svc, X, y=labels, n_jobs=20, return_train_score=True,\n",
    "                           groups=groups, cv=loso, scoring='accuracy')\n",
    "y_pred = cross_val_predict(fs_svc, X, y=labels, n_jobs=20,groups=groups, cv=loso)\n",
    "\n",
    "## Save the SVM weights to a nifti\n",
    "coef = svc.coef_\n",
    "coef = feature_selection.inverse_transform(coef)\n",
    "weight_img = masker.inverse_transform(coef)\n",
    "weight_img.to_filename(output_dir + '/svmweights_'+ analysis +'.nii.gz')\n",
    "\n",
    "## Calculate performance metrics\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "classification_accuracy = cv_scores['test_score'].mean()\n",
    "chance = 1. / len(labels.unique())\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" % \n",
    "      (classification_accuracy, chance))\n",
    "\n",
    "for label in labels.unique():\n",
    "    sensitivity = recall_score(labels,y_pred,labels=[label],average='weighted')\n",
    "    precision = precision_score(labels,y_pred,labels=[label],average='weighted')\n",
    "\n",
    "    results_file.write(\"%s: classification accuracy: %.4f \\n chance level: %f \\n sensitivity: %f \\n precision: %f \\n\" % \n",
    "    (label, classification_accuracy, chance, sensitivity, precision))\n",
    "\n",
    "# compute and display a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import set_printoptions\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cnf_matrix = confusion_matrix(labels, y_pred)\n",
    "set_printoptions(precision=2)\n",
    "classes = labels.unique()\n",
    "\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    from numpy import arange\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size=16)\n",
    "    plt.yticks(tick_marks, classes, size=16)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j],  'd'),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black', size=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size=16)\n",
    "    plt.xlabel('Predicted label', size=16)\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes)\n",
    "plt.savefig(output_dir + '/confusion_matrix_' + analysis + '.svg', transparent=True)\n",
    "plt.close()\n",
    "\n",
    "results_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
