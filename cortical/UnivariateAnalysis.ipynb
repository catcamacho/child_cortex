{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "from nipype.interfaces.fsl.utils import Merge, Split\n",
    "from nipype.interfaces.fsl.model import Randomise, Cluster, GLM\n",
    "from nipype.interfaces.fsl.maths import ApplyMask\n",
    "\n",
    "# FSL set up- change default file output type\n",
    "from nipype.interfaces.fsl import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI_GZ')\n",
    "\n",
    "#freesurfer setup\n",
    "fs_dir = '/moochie/Cat/Aggregate_anats/subjects_dir'\n",
    "FSCommand.set_default_subjects_dir(fs_dir)\n",
    "\n",
    "#other study-specific variables\n",
    "project_home = '/home/camachocm2/Analysis/aggregate_anats'\n",
    "workflow_dir = project_home + '/workflows'\n",
    "preproc_dir = project_home + '/proc/subj_data'\n",
    "output_dir = project_home + '/proc/group_data'\n",
    "template = proc_dir + '/sample_template/lcbd_template0.nii.gz'\n",
    "mask = project_home + '/sample_template/lcbd_template_mask.nii.gz'\n",
    "\n",
    "# Files for group level analysis\n",
    "group_mat = project_home + '/misc/design.mat'\n",
    "t_contrasts = project_home + '/misc/tcon.con'\n",
    "\n",
    "subject_info = read_csv(analysis_home + '/misc/subject_info.csv', index_col=None)\n",
    "subjects_list = subject_info['freesurferID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling Nodes\n",
    "\n",
    "datasink = Node(DataSink(), name='datasink')\n",
    "datasink.inputs.base_directory = output_dir\n",
    "datasink.inputs.container = output_dir\n",
    "\n",
    "grabdata = Node(DataGrabber(template=preproc_dir + '/Final_CT_templateSpace/*/antsCT_CorticalThickness_trans.nii.gz', \n",
    "                               sort_filelist=True, \n",
    "                               outfields=['data_list']), \n",
    "                   name='grabdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_peaks(clusters_file, stat_file):\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, unravel_index, max\n",
    "    from os.path import abspath\n",
    "    \n",
    "    # load up clusters\n",
    "    clusters_nii = load(clusters_file)\n",
    "    clusters_data = clusters_nii.get_data()\n",
    "    cluster_labels, cluster_sizes = unique(clusters_data, return_counts=True)\n",
    "    cluster_sizes = cluster_sizes[cluster_labels>0]\n",
    "    cluster_labels = cluster_labels[cluster_labels>0]\n",
    "    \n",
    "    # set up dataframe\n",
    "    cluster_info = DataFrame(columns=['clust_num','peak','num_voxels','X','Y','Z'])\n",
    "    cluster_info['clust_num'] = Series(cluster_labels,index=None)\n",
    "    \n",
    "    for i in range(0,len(cluster_labels)):\n",
    "        # load up stat image\n",
    "        stat_nii = load(stat_file)\n",
    "        stat_data = stat_nii.get_data()\n",
    "        stat_data[clusters_data!=cluster_labels[i]]=0\n",
    "        location=unravel_index(stat_data.argmax(), stat_data.shape)\n",
    "        cluster_info.iloc[i,0]=cluster_labels[i]\n",
    "        cluster_info.iloc[i,1]=max(stat_data)\n",
    "        cluster_info.iloc[i,2]=cluster_sizes[i]\n",
    "        cluster_info.iloc[i,3]=location[0]\n",
    "        cluster_info.iloc[i,4]=location[1]\n",
    "        cluster_info.iloc[i,5]=location[2]\n",
    "    \n",
    "    out_prefix = clusters_file[:-7]\n",
    "    cluster_info.to_csv(out_prefix + '_peaks.csv')\n",
    "    cluster_info_file = abspath(out_prefix + '_peaks.csv')\n",
    "    return(cluster_info_file)\n",
    "\n",
    "def extract_cluster_betas(cluster_index_file, sample_L1_data, min_clust_size, subject_ids):\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, zeros_like, invert\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    from os.path import abspath, basename\n",
    "    \n",
    "    sample_data = DataFrame(subject_ids, index=None, columns=['Subject'])\n",
    "    \n",
    "    cluster_nifti = load(cluster_index_file)\n",
    "    cluster_data = cluster_nifti.get_data()\n",
    "    clusters, cluster_sizes = unique(cluster_data, return_counts=True)\n",
    "    cluster_sizes = cluster_sizes[clusters>0]\n",
    "    clusters = clusters[clusters>0]\n",
    "    clusters = clusters[cluster_sizes>min_clust_size]\n",
    "    cluster_sizes = cluster_sizes[cluster_sizes>min_clust_size]\n",
    "    ind_filename = basename(cluster_index_file) \n",
    "    out_prefix = ind_filename[:-7]\n",
    "    \n",
    "    for clust_idx in clusters:\n",
    "        temp = zeros_like(cluster_data)\n",
    "        temp[cluster_data==clust_idx] = 1\n",
    "        temp_nii = Nifti1Image(temp,cluster_nifti.affine)\n",
    "        temp_file = 'temp_clust_mask.nii.gz'\n",
    "        save(temp_nii, temp_file)\n",
    "\n",
    "        eb = ImageMeants()\n",
    "        eb.inputs.in_file = sample_L1_data\n",
    "        eb.inputs.mask = temp_file\n",
    "        eb.inputs.out_file = 'L1vals.txt'\n",
    "        eb.run()\n",
    "        L1vals = open('L1vals.txt').read().splitlines()\n",
    "        sample_data['clust' + str(clust_idx)] = Series(L1vals, index=sample_data.index)\n",
    "    \n",
    "    sample_data.to_csv(out_prefix+'_extracted_L1vals.csv')\n",
    "    extracted_L1vals_csv = abspath(out_prefix+'_extracted_L1vals.csv')\n",
    "\n",
    "    return(extracted_L1vals_csv)\n",
    "\n",
    "def calculate_tstat_min(subject_data_list):\n",
    "    from scipy.stats import t\n",
    "    \n",
    "    N_subs = len(subject_data_list)\n",
    "    dof = N_subs - 1\n",
    "    tstat_threshold = t.ppf(1-0.025, dof)\n",
    "    \n",
    "    return(tstat_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Nodes\n",
    "\n",
    "merge = Node(Merge(dimension = 't'), name = 'merge')\n",
    "\n",
    "apply_mask = Node(ApplyMask(mask_file=mask, nan2zeros=True), name='apply_mask')\n",
    "\n",
    "model = Node(GLM(design=group_mat, mask=mask, \n",
    "                 out_p_name='pvals.nii.gz',\n",
    "                 out_t_name='tstat.nii.gz', \n",
    "                 contrasts=t_contrasts), \n",
    "             name='model')\n",
    "\n",
    "split = Node(Split(dimension='t'), name='split')\n",
    "\n",
    "calc_threshold = Node(Function(input_names=['subject_data_list'], \n",
    "                               output_names=['tstat_threshold'], \n",
    "                               function=calculate_tstat_min), \n",
    "                      name='calc_threshold')\n",
    "\n",
    "cluster = MapNode(Cluster(out_localmax_txt_file = 'cluster_stats.txt',\n",
    "                          dlh=4, \n",
    "                          pthreshold=0.05, \n",
    "                          volume=1472512,\n",
    "                          out_index_file='clusters.nii.gz'), \n",
    "                  name='cluster', iterfield=['in_file'])\n",
    "\n",
    "get_peaks = MapNode(Function(input_names=['clusters_file', 'stat_file'], \n",
    "                             output_names=['cluster_info_file'], \n",
    "                             function=get_cluster_peaks), \n",
    "                    name='get_peaks', iterfield=['clusters_file', 'stat_file'])\n",
    "\n",
    "get_L1vals = MapNode(Function(input_names=['cluster_index_file', 'sample_L1_data', \n",
    "                                           'min_clust_size', 'subject_ids'], \n",
    "                              output_names=['extracted_L1vals_csv'], \n",
    "                              function=extract_cluster_betas), \n",
    "                     name='get_L1vals', iterfield=['cluster_index_file'])\n",
    "get_L1vals.inputs.subject_ids = subjects_list\n",
    "get_L1vals.inputs.min_clust_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis workflow\n",
    "\n",
    "grouplevel = Workflow(name='grouplevel')\n",
    "\n",
    "grouplevel.connect([(grabdata, merge,[('data_list', 'in_files')]),\n",
    "                    (merge, model, [('merged_file','in_file')]),\n",
    "                    (grabdata, calc_threshold, [('data_list','subject_data_list')]),\n",
    "                    (calc_threshold, model, [('tstat_threshold','threshold')]),\n",
    "                    \n",
    "                    (model, split, [('out_t','in_file')]),\n",
    "                    (split, cluster, [('out_files','in_file')]),\n",
    "                    (cluster, get_peaks, [('index_file','clusters_file')]),\n",
    "                    (split, get_peaks, [('out_files','stat_file')]),\n",
    "                    (cluster, get_L1vals ,[('index_file','cluster_index_file')]),\n",
    "                    (merge, get_L1vals, [('merged_file','sample_L1_data')]),\n",
    "                    \n",
    "                    (get_peaks, datasink, [('cluster_info_file','cluster_stats')]),\n",
    "                    (get_L1vals, datasink, [('extracted_L1vals_csv','cluster_L1vals')]),\n",
    "                    (model, datasink, [('out_p','pval_files'),\n",
    "                                       ('out_t','tstat_files')]),\n",
    "                    (cluster, datasink, [('index_file','clusters_file')])\n",
    "                   ])\n",
    "\n",
    "grouplevel.base_dir = wkflow_dir\n",
    "grouplevel.write_graph(graph2use='flat')\n",
    "grouplevel.run('MultiProc', plugin_args={'n_procs': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Effects Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "from os.path import join\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "from nipype.interfaces.utility import IdentityInterface, Function\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.interfaces.fsl.utils import Merge, ImageMeants, Split\n",
    "from nipype.interfaces.fsl.model import Randomise, Cluster\n",
    "from nipype.interfaces.freesurfer.model import Binarize\n",
    "from nipype.interfaces.fsl.maths import ApplyMask, Threshold\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "# MATLAB setup - Specify path to current SPM and the MATLAB's default mode\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('~/spm12/toolbox')\n",
    "MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "\n",
    "# FSL set up- change default file output type\n",
    "from nipype.interfaces.fsl import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI')\n",
    "\n",
    "# Set study variables\n",
    "analysis_home = '/Users/catcamacho/Box/LNCD_rewards_connectivity'\n",
    "#analysis_home = '/Volumes/Zeus/Cat'\n",
    "firstlevel_dir = analysis_home + '/proc/firstlevel'\n",
    "secondlevel_dir = analysis_home + '/proc/secondlevel'\n",
    "workflow_dir = analysis_home + '/workflows'\n",
    "template_dir = analysis_home + '/templates'\n",
    "MNI_template = template_dir + '/MNI152_T1_1mm_brain.nii.gz'\n",
    "MNI_mask = template_dir + '/MNI152_T1_3mm_mask.nii.gz'\n",
    "\n",
    "#pull subject info \n",
    "subject_info = analysis_home + '/misc/subjs.csv'\n",
    "\n",
    "conditions = ['punish','neutral']\n",
    "seed_names = ['L_amyg','R_amyg']\n",
    "\n",
    "# Group analysis models (predicting FC)\n",
    "models = ['brain ~ ageMC + sex + ageMC*sex', \n",
    "          'brain ~ invAgeMC + sex + invAgeMC*sex']\n",
    "\n",
    "model_names = ['linearAge', 'inverseAge']\n",
    "\n",
    "terms = ['age', 'sex', 'ageSexInteract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMEM for MRI data (3D nifti data)\n",
    "def mri_lmem(model, mask, subject_dataframe, subject_files, grouping_variable):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "\n",
    "    from os import getcwd\n",
    "    from os.path import abspath\n",
    "    import statsmodels.formula.api as smf\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from numpy import array, empty_like, stack, nditer, zeros_like, zeros\n",
    "    from pandas import DataFrame, read_csv, Series, concat\n",
    "    from warnings import filterwarnings\n",
    "    filterwarnings(\"ignore\")\n",
    "\n",
    "    working_dir = getcwd() + '/'\n",
    "    subj_data = read_csv(subject_dataframe, header=0, index_col=0)\n",
    "\n",
    "    # Load the brain data\n",
    "    brain_niftis = load(subject_files)\n",
    "    brain_data_4D = brain_niftis.get_data()\n",
    "\n",
    "    # Load the mask\n",
    "    mask_nifti = load(mask)\n",
    "    mask = mask_nifti.get_data()\n",
    "\n",
    "    ## Preallocate the output arrays\n",
    "    # for the model\n",
    "    BIC_data = zeros_like(mask).astype(float)\n",
    "    AIC_data = zeros_like(mask).astype(float)\n",
    "    pval_intercept_data = zeros_like(mask).astype(float)\n",
    "    pval_age_data = zeros_like(mask).astype(float)\n",
    "    pval_sex_data = zeros_like(mask).astype(float)\n",
    "    pval_ageSexInteract_data = zeros_like(mask).astype(float)\n",
    "    # per subject\n",
    "    residuals_data = zeros_like(brain_data_4D).astype(float)\n",
    "    pred_values_data = zeros_like(brain_data_4D).astype(float)\n",
    "\n",
    "    # Set up the actual loops to pull in subject data and do the modeling\n",
    "    for x in range(0,mask.shape[0]):\n",
    "        for y in range(0,mask.shape[1]):\n",
    "            for z in range(0,mask.shape[2]):\n",
    "                if mask[x][y][z] == 1:\n",
    "                    voxel = zeros(brain_data_4D.shape[3])\n",
    "                    for a in range(0,brain_data_4D.shape[3]):\n",
    "                        voxel[a] = brain_data_4D[x][y][z][a]\n",
    "                    voxel = Series(voxel, index=subj_data.index, name='brain')\n",
    "                    data = concat([voxel, subj_data],axis=1)\n",
    "                    mlm = smf.mixedlm(model, data, groups=data[grouping_variable])\n",
    "                    mod = mlm.fit()\n",
    "                    pval_intercept_data[x][y][z] = 1 - mod.pvalues[0]\n",
    "                    pval_age_data[x][y][z] = 1 - mod.pvalues[1]\n",
    "                    pval_sex_data[x][y][z] = 1 - mod.pvalues[2]\n",
    "                    pval_ageSexInteract_data[x][y][z] = 1 - mod.pvalues[3]\n",
    "                    BIC_data[x][y][z] = mod.bic\n",
    "                    AIC_data[x][y][z] = mod.aic\n",
    "                    residuals = mod.resid\n",
    "                    pred_values = Series(mod.predict(), index = subj_data.index)\n",
    "                    for d in range(0,brain_data_4D.shape[3]):\n",
    "                        residuals_data[x][y][z][d] = residuals.tolist()[d]\n",
    "                        pred_values_data[x][y][z][d] = pred_values.tolist()[d]\n",
    "\n",
    "                \n",
    "    # Save the ouputs as nifti files\n",
    "    output_data = [BIC_data, AIC_data, pval_intercept_data, pval_age_data,\n",
    "                    pval_sex_data, pval_ageSexInteract_data, residuals_data, \n",
    "                    pred_values_data]\n",
    "    output_niftis = [Nifti1Image(result, mask_nifti.affine) for result in output_data]\n",
    "    \n",
    "    output_filenames = ['BICs.nii','AICs.nii','pval_intercept_data.nii',\n",
    "                        'pval_age_data.nii','pval_sex_data.nii',\n",
    "                        'pval_ageSexInteract_data.nii','residuals_data.nii',\n",
    "                        'pred_values_data.nii']\n",
    "    for e in range(0,len(output_niftis)):\n",
    "        save(output_niftis[e], working_dir + output_filenames[e])\n",
    "    \n",
    "    output_volumes = [abspath(output_filenames[0]),\n",
    "                      abspath(output_filenames[1]),\n",
    "                      abspath(output_filenames[2]), \n",
    "                      abspath(output_filenames[3]), \n",
    "                      abspath(output_filenames[4]), \n",
    "                      abspath(output_filenames[5]), \n",
    "                      abspath(output_filenames[6]), \n",
    "                      abspath(output_filenames[7])]\n",
    "    \n",
    "    return(output_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling nodes\n",
    "\n",
    "conditionsource = Node(IdentityInterface(fields=['condition','seed']),\n",
    "                       name='conditionsource')\n",
    "conditionsource.iterables = [('condition',conditions),('seed', seed_names)]\n",
    "\n",
    "# Grab the subject beta maps \n",
    "time_template = {'beta_maps':firstlevel_dir + '/smoothedMNI_conn_beta/*/%s/%s/betas_flirt_smooth_masked.nii'}\n",
    "betamap_grabber = Node(DataGrabber(sort_filelist=True,\n",
    "                                   field_template = time_template,\n",
    "                                   base_directory=firstlevel_dir,\n",
    "                                   template=firstlevel_dir + '/smoothedMNI_conn_beta/*/%s/%s/betas_flirt_smooth_masked.nii',\n",
    "                                   infields=['condition','seed'],\n",
    "                                   template_args={'beta_maps':[['condition','seed']]}), \n",
    "                       name='betamap_grabber')\n",
    "\n",
    "# Sink relavent data\n",
    "substitutions = [('_condition_',''),\n",
    "                 ('_seed_',''), \n",
    "                 ('brain~ageMC+sex+ageMC*sex','linearAge'),\n",
    "                 ('brain~invAgeMC+sex+invAgeMC*sex','inverseAge')]\n",
    "datasink = Node(DataSink(substitutions=substitutions, \n",
    "                         base_directory=secondlevel_dir,\n",
    "                         container=secondlevel_dir), \n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis nodes\n",
    "\n",
    "#Merge subject files together\n",
    "merge = Node(Merge(dimension='t'), name='merge')\n",
    "\n",
    "# Linear mixed effects modeling\n",
    "lmemodel = Node(Function(input_names = ['model', 'mask', 'subject_dataframe', \n",
    "                                        'subject_files', 'grouping_variable'], \n",
    "                         output_names = ['output_volumes'], \n",
    "                         function=mri_lmem), \n",
    "                name='lmemodel')\n",
    "lmemodel.iterables = [('model', models)]\n",
    "lmemodel.inputs.mask = MNI_mask\n",
    "lmemodel.inputs.subject_dataframe = subject_info\n",
    "lmemodel.inputs.grouping_variable = 'Timepoint'\n",
    "\n",
    "# Mask the file to only significant voxels for clustering\n",
    "mask_stat = Node(Binarize(), name = 'mask_stat')\n",
    "\n",
    "# Cluster the results\n",
    "cluster_results = MapNode(Cluster(threshold=0.95,\n",
    "                                  out_index_file=True,\n",
    "                                  out_localmax_txt_file=True),\n",
    "                          name='cluster_results', \n",
    "                          iterfield = ['in_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LMEManalysisflow = Workflow(name='LMEManalysisflow')\n",
    "LMEManalysisflow.connect([(conditionsource, betamap_grabber, [('condition','condition'),\n",
    "                                                              ('seed','seed')]),\n",
    "                          (betamap_grabber, merge, [('beta_maps','in_files')]),\n",
    "                          (merge, lmemodel, [('merged_file','subject_files')]),\n",
    "                          (merge, datasink, [('merged_file','merged_subj_betas')]),\n",
    "                          (lmemodel, datasink, [('output_volumes','output_volumes')])\n",
    "                         ])\n",
    "#LMEManalysisflow.base_dir = workflow_dir\n",
    "#LMEManalysisflow.write_graph(graph2use='flat')\n",
    "#LMEManalysisflow.run('MultiProc', plugin_args={'n_procs':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_clusters(cluster_index):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "\n",
    "    from os import getcwd\n",
    "    from os.path import abspath\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from numpy import unique, amax, stack, zeros_like\n",
    "\n",
    "    cluster_min = 20\n",
    "    nifti = load(cluster_index)\n",
    "    data = nifti.get_data()\n",
    "    clust_labels, vox_count = unique(data, return_counts=True)\n",
    "    clust_labels = clust_labels.tolist()\n",
    "    vox_count = vox_count.tolist()\n",
    "\n",
    "    iter = 0\n",
    "    for i in range(0, len(clust_labels)):\n",
    "        if vox_count[i] < cluster_min:\n",
    "            del(clust_labels[i-iter])\n",
    "            iter = iter +1\n",
    "\n",
    "    clust_labels = clust_labels[1:]\n",
    "    num_clusters = len(clust_labels)\n",
    "\n",
    "    cluster_masks = []\n",
    "    for a in range(0,num_clusters):\n",
    "        temp = zeros_like(data)\n",
    "        temp[data==clust_labels[a]] = 1\n",
    "        cluster_masks.append(temp)\n",
    "\n",
    "    cluster_masks_4D = stack(cluster_masks,axis=3)\n",
    "    cluster_masks_nifti = Nifti1Image(cluster_masks_4D, nifti.affine)\n",
    "    save(cluster_masks_nifti, 'new_clust_index.nii')\n",
    "    newcluster_index = abspath('new_clust_index.nii')\n",
    "    return(newcluster_index)\n",
    "\n",
    "\n",
    "def finalize_models(cluster, template, betas_text_file, subj_data, model_name):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import abspath\n",
    "    import statsmodels.formula.api as smf\n",
    "    from pandas import DataFrame, Series, concat, read_table, read_csv\n",
    "    from ggplot import *\n",
    "    from warnings import filterwarnings\n",
    "    filterwarnings(\"ignore\")\n",
    "    import sys\n",
    "    from nilearn import plotting\n",
    "    \n",
    "    #determine which model to use\n",
    "    if model_name=='linearAge':\n",
    "        model = 'brain ~ ageMC + sex + ageMC*sex'\n",
    "    elif model_name=='inverseAge':\n",
    "        model = 'brain ~ invAgeMC + sex + invAgeMC*sex'\n",
    "        \n",
    "    origstdout = sys.stdout\n",
    "    sys.stdout = open('modelsummary.txt', 'w')\n",
    "\n",
    "    #organize data into dataframe for modeling\n",
    "    subj_data = read_csv(subj_data, header=0, index_col=0)\n",
    "    brain_data = read_table(betas_text_file, header=None, names='brain', index_col=None)\n",
    "    data = concat([brain_data, subj_data],axis=1)\n",
    "    # do the modeling\n",
    "    mlm = smf.mixedlm(model, data, groups=data[grouping_variable])\n",
    "    mod = mlm.fit()\n",
    "    print(mod.summary())\n",
    "    sys.stdout = origstdout\n",
    "    close('modelsummary.txt')\n",
    "    summary_file = abspath('modelsummary.txt')\n",
    "    \n",
    "    # plot the model results\n",
    "    figure = ggplot(data, aes(x='age',y='brain') + theme_classic() + \n",
    "                    geom_point() + geom_smooth(method=lm,se=True, size=2))\n",
    "    figure.save('plot.svg')\n",
    "    figure_file = abspath('plot.svg')\n",
    "    \n",
    "    # make a picture of the brain cluster\n",
    "    display = plotting.plot_anat(anat_img = template, display_mode='x')\n",
    "    display.add_overlay(cluster, plotting.cm.purple_green, threshold=1)\n",
    "    display.savefig('clusterpic.png')\n",
    "    display.close()\n",
    "    clusterpic_file = abspath('clusterpic.png')\n",
    "    \n",
    "    outputs = [summary_file, figure_file, clusterpic_file]\n",
    "    \n",
    "    return(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data outputs from the LMEMs\n",
    "infosource = Node(IdentityInterface(fields=['condition','seed','term','model']),\n",
    "                  name='conditionsource')\n",
    "infosource.iterables = [('condition', conditions), \n",
    "                        ('seed', seed_names), \n",
    "                        ('term', terms), \n",
    "                        ('model', model_names)]\n",
    "\n",
    "lme_template = {'pval_vol': secondlevel_dir + '/output_volumes/{condition}{seed}/_model_{model}/pval_{term}_data.nii', \n",
    "                'subj_beta_data': secondlevel_dir + '/merged_betas/betas_merged.nii'}            \n",
    "lme_datagrabber = Node(SelectFiles(lme_template), name='lme_datagrabber') \n",
    "\n",
    "# Threshold out nonsignificant voxels\n",
    "threshold = Node(Threshold(thresh=0.95), name='threshold')\n",
    "\n",
    "# Cluster the remaining volumes\n",
    "cluster = Node(Cluster(threshold=0.95, \n",
    "                       out_index_file=True, \n",
    "                       use_mm=True,\n",
    "                       minclustersize=True,\n",
    "                       peak_distance=6), \n",
    "               name='cluster')\n",
    "\n",
    "# remove small clusters\n",
    "cluster_min = Node(Function(input_names=['cluster_index'], \n",
    "                            output_names=['newcluster_index'], \n",
    "                            function=threshold_clusters), \n",
    "                   name='cluster_min')\n",
    "\n",
    "# Split the clusters\n",
    "split = Node(Split(dimension='t'),\n",
    "             name='split')\n",
    "\n",
    "# Extract mean connectivity per cluster\n",
    "pull_mean_betas = MapNode(ImageMeants(out_file='mean_connectivity.txt'), \n",
    "                          name= 'pull_mean_betas', \n",
    "                          iterfield=['mask'])\n",
    "                         \n",
    "\n",
    "# graph the connectivity against age and re-do the linear models\n",
    "finalize_models = MapNode(Function(input_names=['cluster','template','betas_text_file','subj_data', 'model_name'], \n",
    "                                   output_names=['outputs'], \n",
    "                                   function=finalize_models),\n",
    "                          name='finalize_models', \n",
    "                          iterfield=['betas_text_file'])\n",
    "finalize_models.inputs.subj_data=subject_info\n",
    "finalize_models.inputs.template=MNI_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusterflow = Workflow(name='clusterflow')\n",
    "clusterflow.connect([(infosource, lme_datagrabber, [('condition','condition'),\n",
    "                                                    ('seed','seed'),\n",
    "                                                    ('term','term'),\n",
    "                                                    ('model','model')]),\n",
    "                     (lme_datagrabber,threshold, [('pval_vol','in_file')]),\n",
    "                     (threshold, cluster, [('out_file','in_file')]),\n",
    "                     (cluster, cluster_min, [('index_file','cluster_index')]),\n",
    "                     (cluster_min, split, [('newcluster_index','in_file')]),\n",
    "                     (split, pull_mean_betas, [('out_files','mask')]),\n",
    "                     (lme_datagrabber, pull_mean_betas, [('subj_beta_data','in_file')]),\n",
    "                     \n",
    "                     (pull_mean_betas, datasink, [('out_file','beta_values')]),\n",
    "                     (cluster, datasink, [('index_file','cluster_index_file'),\n",
    "                                          ('localmax_txt_file','cluster_localmax_txt_file')]),\n",
    "                     (split,datasink,[('out_files','final_clusters')])\n",
    "                    ])\n",
    "clusterflow.base_dir = workflow_dir\n",
    "clusterflow.write_graph(graph2use='flat')\n",
    "clusterflow.run('MultiProc', plugin_args={'n_procs':2})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
