{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "from nipype.interfaces.fsl.utils import Merge, Split, Smooth\n",
    "from nipype.interfaces.fsl.model import Randomise, Cluster, GLM\n",
    "from nipype.interfaces.freesurfer import FSCommand\n",
    "from pandas import read_csv\n",
    "\n",
    "# FSL set up- change default file output type\n",
    "from nipype.interfaces.fsl import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI_GZ')\n",
    "\n",
    "#freesurfer setup\n",
    "fs_dir = '/moochie/Cat/Aggregate_anats/subjects_dir'\n",
    "FSCommand.set_default_subjects_dir(fs_dir)\n",
    "\n",
    "#other study-specific variables\n",
    "project_home = '/home/camachocm2/Analysis/aggregate_anats'\n",
    "workflow_dir = project_home + '/workflows'\n",
    "preproc_dir = project_home + '/proc/subj_data'\n",
    "output_dir = project_home + '/proc/group_data/incomepc'\n",
    "template = preproc_dir + '/sample_template/lcbd_template0.nii.gz'\n",
    "mask = preproc_dir + '/sample_template/lcbd_template_mask.nii.gz'\n",
    "\n",
    "# Files for group level analysis\n",
    "t_contrasts = project_home + '/misc/tcon_income.con'\n",
    "\n",
    "subject_info = read_csv(project_home + '/results/combined_data_20190311.csv', index_col=0)\n",
    "subjects_list = subject_info['freesurferID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling Nodes\n",
    "\n",
    "datasink = Node(DataSink(), name='datasink')\n",
    "datasink.inputs.base_directory = output_dir\n",
    "datasink.inputs.container = output_dir\n",
    "\n",
    "grabdata = Node(DataGrabber(template=preproc_dir + '/Final_CT_templateSpace/*/antsCT_CorticalThickness_trans.nii.gz', \n",
    "                               sort_filelist=True, \n",
    "                               outfields=['data_list']), \n",
    "                   name='grabdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_peaks(clusters_file, stat_file):\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, unravel_index, max\n",
    "    from os.path import abspath\n",
    "    \n",
    "    # load up clusters\n",
    "    clusters_nii = load(clusters_file)\n",
    "    clusters_data = clusters_nii.get_data()\n",
    "    cluster_labels, cluster_sizes = unique(clusters_data, return_counts=True)\n",
    "    cluster_sizes = cluster_sizes[cluster_labels>0]\n",
    "    cluster_labels = cluster_labels[cluster_labels>0]\n",
    "    \n",
    "    # set up dataframe\n",
    "    cluster_info = DataFrame(columns=['clust_num','peak','num_voxels','X','Y','Z'])\n",
    "    cluster_info['clust_num'] = Series(cluster_labels,index=None)\n",
    "    \n",
    "    for i in range(0,len(cluster_labels)):\n",
    "        # load up stat image\n",
    "        stat_nii = load(stat_file)\n",
    "        stat_data = stat_nii.get_data()\n",
    "        stat_data[clusters_data!=cluster_labels[i]]=0\n",
    "        location=unravel_index(stat_data.argmax(), stat_data.shape)\n",
    "        cluster_info.iloc[i,0]=cluster_labels[i]\n",
    "        cluster_info.iloc[i,1]=max(stat_data)\n",
    "        cluster_info.iloc[i,2]=cluster_sizes[i]\n",
    "        cluster_info.iloc[i,3]=location[0]\n",
    "        cluster_info.iloc[i,4]=location[1]\n",
    "        cluster_info.iloc[i,5]=location[2]\n",
    "    \n",
    "    out_prefix = clusters_file[:-7]\n",
    "    cluster_info.to_csv(out_prefix + '_peaks.csv')\n",
    "    cluster_info_file = abspath(out_prefix + '_peaks.csv')\n",
    "    return(cluster_info_file)\n",
    "\n",
    "def extract_cluster_betas(cluster_index_file, sample_L1_data, min_clust_size, subject_ids):\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from pandas import DataFrame, Series\n",
    "    from numpy import unique, zeros_like, invert\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    from os.path import abspath, basename\n",
    "    \n",
    "    sample_data = DataFrame(subject_ids, index=None, columns=['Subject'])\n",
    "    \n",
    "    cluster_nifti = load(cluster_index_file)\n",
    "    cluster_data = cluster_nifti.get_data()\n",
    "    clusters, cluster_sizes = unique(cluster_data, return_counts=True)\n",
    "    cluster_sizes = cluster_sizes[clusters>0]\n",
    "    clusters = clusters[clusters>0]\n",
    "    clusters = clusters[cluster_sizes>min_clust_size]\n",
    "    cluster_sizes = cluster_sizes[cluster_sizes>min_clust_size]\n",
    "    ind_filename = basename(cluster_index_file) \n",
    "    out_prefix = ind_filename[:-7]\n",
    "    \n",
    "    for clust_idx in clusters:\n",
    "        temp = zeros_like(cluster_data)\n",
    "        temp[cluster_data==clust_idx] = 1\n",
    "        temp_nii = Nifti1Image(temp,cluster_nifti.affine)\n",
    "        temp_file = 'temp_clust_mask.nii.gz'\n",
    "        save(temp_nii, temp_file)\n",
    "\n",
    "        eb = ImageMeants()\n",
    "        eb.inputs.in_file = sample_L1_data\n",
    "        eb.inputs.mask = temp_file\n",
    "        eb.inputs.out_file = 'L1vals.txt'\n",
    "        eb.run()\n",
    "        L1vals = open('L1vals.txt').read().splitlines()\n",
    "        sample_data['clust' + str(clust_idx)] = Series(L1vals, index=sample_data.index)\n",
    "    \n",
    "    sample_data.to_csv(out_prefix+'_extracted_L1vals.csv')\n",
    "    extracted_L1vals_csv = abspath(out_prefix+'_extracted_L1vals.csv')\n",
    "\n",
    "    return(extracted_L1vals_csv)\n",
    "\n",
    "def calculate_tstat_min(subject_data_list):\n",
    "    from scipy.stats import t\n",
    "    \n",
    "    N_subs = len(subject_data_list)\n",
    "    dof = N_subs - 1\n",
    "    tstat_threshold = t.ppf(1-0.001, dof)\n",
    "    \n",
    "    return(tstat_threshold)\n",
    "\n",
    "def sub_lists(subjects_df, data_list):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from subprocess import check_call\n",
    "    from os.path import abspath\n",
    "    from nipype.interfaces.fsl import Merge\n",
    "    \n",
    "    data_list = sorted(data_list)\n",
    "    usable_subs = subjects_df[subjects_df['CT_QC']==1]\n",
    "    subjects_list = usable_subs['freesurferID'].tolist()\n",
    "    Factor1 = usable_subs['Factor1'].tolist()\n",
    "    Factor2 = usable_subs['Factor2'].tolist()\n",
    "    Factor3 = usable_subs['Factor3'].tolist()\n",
    "    Factor4 = usable_subs['Factor4'].tolist()\n",
    "    subjects_ages = usable_subs['Age_yrs'].tolist()\n",
    "    male = usable_subs['male'].tolist()\n",
    "    seq1 = usable_subs['seq1'].tolist()\n",
    "    seq2 = usable_subs['seq2'].tolist()\n",
    "    seq3 = usable_subs['seq3'].tolist()\n",
    "    seq4 = usable_subs['seq4'].tolist()\n",
    "    eTIV = usable_subs['eTIV'].tolist()\n",
    "\n",
    "    final_data_list = []\n",
    "    final_subjects_list = []\n",
    "    text_file = open('temp_text.txt','w')\n",
    "\n",
    "    for a in range(0,len(subjects_list)):\n",
    "        for file in data_list:\n",
    "            if '/' + subjects_list[a] + '/' in file:\n",
    "                final_data_list.append(file)\n",
    "                final_subjects_list.append(subjects_list[a])\n",
    "                text_file.write('{0} {1} {2} {3} {4} {5} {6} {7} {8} {9} {10}\\n'.format(Factor1[a], Factor2[a], Factor3[a], \n",
    "                                                                                        Factor4[a],subjects_ages[a], male[a], \n",
    "                                                                                        seq1[a], seq2[a], seq3[a],seq4[a], \n",
    "                                                                                        eTIV[a]))\n",
    "                #text_file.write('{0} {1} {2} {3} {4} {5} {6} {7} {8}\\n'.format(Factor1[a], Factor3[a], subjects_ages[a],\n",
    "                #                                                               male[a], seq1[a], seq2[a], seq3[a],seq4[a],\n",
    "                #                                                               eTIV[a]))\n",
    "                \n",
    "    text_file.close()\n",
    "\n",
    "    file = abspath('temp_text.txt')\n",
    "    check_call(['Text2Vest',file,'design.mat'])\n",
    "    design_file = abspath('design.mat')\n",
    "\n",
    "    me=Merge()\n",
    "    me.inputs.dimension='t'\n",
    "    me.inputs.in_files=final_data_list\n",
    "    me.inputs.merged_file='data_merged.nii.gz'\n",
    "    me.run()\n",
    "\n",
    "    final_data = abspath('data_merged.nii.gz')\n",
    "\n",
    "    return(final_data,design_file,final_subjects_list)\n",
    "\n",
    "def sub_lists_income(subjects_df, data_list):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from subprocess import check_call\n",
    "    from os.path import abspath\n",
    "    from nipype.interfaces.fsl import Merge\n",
    "    \n",
    "    data_list = sorted(data_list)\n",
    "    usable_subs = subjects_df[subjects_df['CT_QC_income']==1]\n",
    "    subjects_list = usable_subs['freesurferID'].tolist()\n",
    "    Factor1 = usable_subs['incomeperchild'].tolist()\n",
    "    subjects_ages = usable_subs['Age_yrs'].tolist()\n",
    "    male = usable_subs['male'].tolist()\n",
    "    seq1 = usable_subs['seq1'].tolist()\n",
    "    seq2 = usable_subs['seq2'].tolist()\n",
    "    seq3 = usable_subs['seq3'].tolist()\n",
    "    seq4 = usable_subs['seq4'].tolist()\n",
    "    eTIV = usable_subs['eTIV'].tolist()\n",
    "\n",
    "    final_data_list = []\n",
    "    final_subjects_list = []\n",
    "    text_file = open('temp_text.txt','w')\n",
    "\n",
    "    for a in range(0,len(subjects_list)):\n",
    "        for file in data_list:\n",
    "            if '/' + subjects_list[a] + '/' in file:\n",
    "                final_data_list.append(file)\n",
    "                final_subjects_list.append(subjects_list[a])\n",
    "                text_file.write('{0} {1} {2} {3} {4} {5} {6} {7}\\n'.format(Factor1[a], subjects_ages[a], male[a], \n",
    "                                                                                        seq1[a], seq2[a], seq3[a],seq4[a], \n",
    "                                                                                        eTIV[a]))\n",
    "                \n",
    "    text_file.close()\n",
    "\n",
    "    file = abspath('temp_text.txt')\n",
    "    check_call(['Text2Vest',file,'design.mat'])\n",
    "    design_file = abspath('design.mat')\n",
    "\n",
    "    me=Merge()\n",
    "    me.inputs.dimension='t'\n",
    "    me.inputs.in_files=final_data_list\n",
    "    me.inputs.merged_file='data_merged.nii.gz'\n",
    "    me.run()\n",
    "\n",
    "    final_data = abspath('data_merged.nii.gz')\n",
    "\n",
    "    return(final_data,design_file,final_subjects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Nodes\n",
    "\n",
    "select_usable_data = Node(Function(input_names=['subjects_df', 'data_list'],\n",
    "                                   output_names=['final_data','design_file','final_subjects_list'], \n",
    "                                   function=sub_lists_income), \n",
    "                          name = 'select_usable_data')\n",
    "select_usable_data.inputs.subjects_df = subject_info\n",
    "\n",
    "smooth = Node(Smooth(fwhm=4),name='smooth')\n",
    "\n",
    "#model = Node(GLM(mask=mask, \n",
    "#                 out_p_name='pvals.nii.gz',\n",
    "#                 out_t_name='tstat.nii.gz', \n",
    "#                 contrasts=t_contrasts), \n",
    "#             name='model')\n",
    "\n",
    "model = Node(Randomise(mask=mask, \n",
    "                       tfce=False,\n",
    "                       num_perm=1000,\n",
    "                       tcon=t_contrasts), \n",
    "             name='model')\n",
    "\n",
    "split = Node(Split(dimension='t'), name='split')\n",
    "\n",
    "calc_threshold = Node(Function(input_names=['subject_data_list'], \n",
    "                               output_names=['tstat_threshold'], \n",
    "                               function=calculate_tstat_min), \n",
    "                      name='calc_threshold')\n",
    "\n",
    "cluster = MapNode(Cluster(out_localmax_txt_file = 'cluster_stats.txt',\n",
    "                          dlh=4, \n",
    "                          pthreshold=0.05, \n",
    "                          volume=1472512,\n",
    "                          out_index_file='clusters.nii.gz'), \n",
    "                  name='cluster', iterfield=['in_file'])\n",
    "\n",
    "get_peaks = MapNode(Function(input_names=['clusters_file', 'stat_file'], \n",
    "                             output_names=['cluster_info_file'], \n",
    "                             function=get_cluster_peaks), \n",
    "                    name='get_peaks', iterfield=['clusters_file', 'stat_file'])\n",
    "\n",
    "get_L1vals = MapNode(Function(input_names=['cluster_index_file', 'sample_L1_data', \n",
    "                                           'min_clust_size', 'subject_ids'], \n",
    "                              output_names=['extracted_L1vals_csv'], \n",
    "                              function=extract_cluster_betas), \n",
    "                     name='get_L1vals', iterfield=['cluster_index_file'])\n",
    "get_L1vals.inputs.min_clust_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis workflow\n",
    "\n",
    "grouplevel = Workflow(name='grouplevel_incomepc')\n",
    "\n",
    "grouplevel.connect([(grabdata, select_usable_data,[('data_list', 'data_list')]),\n",
    "                    (select_usable_data, smooth, [('final_data','in_file')]),\n",
    "                    (smooth, model, [('smoothed_file','in_file')]),\n",
    "                    (select_usable_data, model, [('design_file','design_mat')]),\n",
    "                    (select_usable_data, calc_threshold, [('final_subjects_list','subject_data_list')]),\n",
    "                    (select_usable_data, get_L1vals, [('final_subjects_list','subject_ids')]),\n",
    "                    (calc_threshold, cluster, [('tstat_threshold','threshold')]),\n",
    "                    \n",
    "                    (model, cluster, [('tstat_files','in_file')]),\n",
    "                    (cluster, get_peaks, [('index_file','clusters_file')]),\n",
    "                    (model, get_peaks, [('tstat_files','stat_file')]),\n",
    "                    (cluster, get_L1vals ,[('index_file','cluster_index_file')]),\n",
    "                    (select_usable_data, get_L1vals, [('final_data','sample_L1_data')]),\n",
    "                    \n",
    "                    (get_peaks, datasink, [('cluster_info_file','cluster_stats')]),\n",
    "                    (get_L1vals, datasink, [('extracted_L1vals_csv','cluster_L1vals')]),\n",
    "                    (model, datasink, [('tstat_files','tstat_files')]),\n",
    "                    (cluster, datasink, [('index_file','clusters_file')])\n",
    "                   ])\n",
    "\n",
    "grouplevel.base_dir = workflow_dir\n",
    "grouplevel.write_graph(graph2use='flat')\n",
    "grouplevel.run('MultiProc', plugin_args={'n_procs': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Effects Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "from os.path import join\n",
    "from nipype.pipeline.engine import Workflow, Node, MapNode\n",
    "from nipype.interfaces.utility import IdentityInterface, Function\n",
    "from nipype.interfaces.io import SelectFiles, DataSink, DataGrabber\n",
    "from nipype.interfaces.fsl.utils import Merge, ImageMeants, Split\n",
    "from nipype.interfaces.fsl.model import Randomise, Cluster\n",
    "from nipype.interfaces.freesurfer.model import Binarize\n",
    "from nipype.interfaces.fsl.maths import ApplyMask, Threshold\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "# MATLAB setup - Specify path to current SPM and the MATLAB's default mode\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('~/spm12/toolbox')\n",
    "MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "\n",
    "# FSL set up- change default file output type\n",
    "from nipype.interfaces.fsl import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI')\n",
    "\n",
    "# Set study variables\n",
    "analysis_home = '/Users/catcamacho/Box/LNCD_rewards_connectivity'\n",
    "firstlevel_dir = analysis_home + '/proc/firstlevel'\n",
    "secondlevel_dir = analysis_home + '/proc/secondlevel'\n",
    "workflow_dir = analysis_home + '/workflows'\n",
    "template_dir = analysis_home + '/templates'\n",
    "MNI_template = template_dir + '/MNI152_T1_1mm_brain.nii.gz'\n",
    "MNI_mask = template_dir + '/MNI152_T1_3mm_mask.nii.gz'\n",
    "\n",
    "#pull subject info \n",
    "subject_info = analysis_home + '/misc/subjs.csv'\n",
    "\n",
    "conditions = ['punish','neutral']\n",
    "seed_names = ['L_amyg','R_amyg']\n",
    "\n",
    "# Group analysis models (predicting FC)\n",
    "models = ['brain ~ ageMC + sex + ageMC*sex', \n",
    "          'brain ~ invAgeMC + sex + invAgeMC*sex']\n",
    "\n",
    "model_names = ['linearAge', 'inverseAge']\n",
    "\n",
    "terms = ['age', 'sex', 'ageSexInteract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMEM for MRI data (3D nifti data)\n",
    "def mri_lmem(model, mask, subject_dataframe, subject_files, grouping_variable):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "\n",
    "    from os import getcwd\n",
    "    from os.path import abspath\n",
    "    import statsmodels.formula.api as smf\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from numpy import array, empty_like, stack, nditer, zeros_like, zeros\n",
    "    from pandas import DataFrame, read_csv, Series, concat\n",
    "    from warnings import filterwarnings\n",
    "    filterwarnings(\"ignore\")\n",
    "\n",
    "    working_dir = getcwd() + '/'\n",
    "    subj_data = read_csv(subject_dataframe, header=0, index_col=0)\n",
    "\n",
    "    # Load the brain data\n",
    "    brain_niftis = load(subject_files)\n",
    "    brain_data_4D = brain_niftis.get_data()\n",
    "\n",
    "    # Load the mask\n",
    "    mask_nifti = load(mask)\n",
    "    mask = mask_nifti.get_data()\n",
    "\n",
    "    ## Preallocate the output arrays\n",
    "    # for the model\n",
    "    BIC_data = zeros_like(mask).astype(float)\n",
    "    AIC_data = zeros_like(mask).astype(float)\n",
    "    pval_intercept_data = zeros_like(mask).astype(float)\n",
    "    pval_age_data = zeros_like(mask).astype(float)\n",
    "    pval_sex_data = zeros_like(mask).astype(float)\n",
    "    pval_ageSexInteract_data = zeros_like(mask).astype(float)\n",
    "    # per subject\n",
    "    residuals_data = zeros_like(brain_data_4D).astype(float)\n",
    "    pred_values_data = zeros_like(brain_data_4D).astype(float)\n",
    "\n",
    "    # Set up the actual loops to pull in subject data and do the modeling\n",
    "    for x in range(0,mask.shape[0]):\n",
    "        for y in range(0,mask.shape[1]):\n",
    "            for z in range(0,mask.shape[2]):\n",
    "                if mask[x][y][z] == 1:\n",
    "                    voxel = zeros(brain_data_4D.shape[3])\n",
    "                    for a in range(0,brain_data_4D.shape[3]):\n",
    "                        voxel[a] = brain_data_4D[x][y][z][a]\n",
    "                    voxel = Series(voxel, index=subj_data.index, name='brain')\n",
    "                    data = concat([voxel, subj_data],axis=1)\n",
    "                    mlm = smf.mixedlm(model, data, groups=data[grouping_variable])\n",
    "                    mod = mlm.fit()\n",
    "                    pval_intercept_data[x][y][z] = 1 - mod.pvalues[0]\n",
    "                    pval_age_data[x][y][z] = 1 - mod.pvalues[1]\n",
    "                    pval_sex_data[x][y][z] = 1 - mod.pvalues[2]\n",
    "                    pval_ageSexInteract_data[x][y][z] = 1 - mod.pvalues[3]\n",
    "                    BIC_data[x][y][z] = mod.bic\n",
    "                    AIC_data[x][y][z] = mod.aic\n",
    "                    residuals = mod.resid\n",
    "                    pred_values = Series(mod.predict(), index = subj_data.index)\n",
    "                    for d in range(0,brain_data_4D.shape[3]):\n",
    "                        residuals_data[x][y][z][d] = residuals.tolist()[d]\n",
    "                        pred_values_data[x][y][z][d] = pred_values.tolist()[d]\n",
    "\n",
    "                \n",
    "    # Save the ouputs as nifti files\n",
    "    output_data = [BIC_data, AIC_data, pval_intercept_data, pval_age_data,\n",
    "                    pval_sex_data, pval_ageSexInteract_data, residuals_data, \n",
    "                    pred_values_data]\n",
    "    output_niftis = [Nifti1Image(result, mask_nifti.affine) for result in output_data]\n",
    "    \n",
    "    output_filenames = ['BICs.nii','AICs.nii','pval_intercept_data.nii',\n",
    "                        'pval_age_data.nii','pval_sex_data.nii',\n",
    "                        'pval_ageSexInteract_data.nii','residuals_data.nii',\n",
    "                        'pred_values_data.nii']\n",
    "    for e in range(0,len(output_niftis)):\n",
    "        save(output_niftis[e], working_dir + output_filenames[e])\n",
    "    \n",
    "    output_volumes = [abspath(output_filenames[0]),\n",
    "                      abspath(output_filenames[1]),\n",
    "                      abspath(output_filenames[2]), \n",
    "                      abspath(output_filenames[3]), \n",
    "                      abspath(output_filenames[4]), \n",
    "                      abspath(output_filenames[5]), \n",
    "                      abspath(output_filenames[6]), \n",
    "                      abspath(output_filenames[7])]\n",
    "    \n",
    "    return(output_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handling nodes\n",
    "\n",
    "conditionsource = Node(IdentityInterface(fields=['condition','seed']),\n",
    "                       name='conditionsource')\n",
    "conditionsource.iterables = [('condition',conditions),('seed', seed_names)]\n",
    "\n",
    "# Grab the subject beta maps \n",
    "time_template = {'beta_maps':firstlevel_dir + '/smoothedMNI_conn_beta/*/%s/%s/betas_flirt_smooth_masked.nii'}\n",
    "betamap_grabber = Node(DataGrabber(sort_filelist=True,\n",
    "                                   field_template = time_template,\n",
    "                                   base_directory=firstlevel_dir,\n",
    "                                   template=firstlevel_dir + '/smoothedMNI_conn_beta/*/%s/%s/betas_flirt_smooth_masked.nii',\n",
    "                                   infields=['condition','seed'],\n",
    "                                   template_args={'beta_maps':[['condition','seed']]}), \n",
    "                       name='betamap_grabber')\n",
    "\n",
    "# Sink relavent data\n",
    "substitutions = [('_condition_',''),\n",
    "                 ('_seed_',''), \n",
    "                 ('brain~ageMC+sex+ageMC*sex','linearAge'),\n",
    "                 ('brain~invAgeMC+sex+invAgeMC*sex','inverseAge')]\n",
    "datasink = Node(DataSink(substitutions=substitutions, \n",
    "                         base_directory=secondlevel_dir,\n",
    "                         container=secondlevel_dir), \n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis nodes\n",
    "\n",
    "#Merge subject files together\n",
    "merge = Node(Merge(dimension='t'), name='merge')\n",
    "\n",
    "# Linear mixed effects modeling\n",
    "lmemodel = Node(Function(input_names = ['model', 'mask', 'subject_dataframe', \n",
    "                                        'subject_files', 'grouping_variable'], \n",
    "                         output_names = ['output_volumes'], \n",
    "                         function=mri_lmem), \n",
    "                name='lmemodel')\n",
    "lmemodel.iterables = [('model', models)]\n",
    "lmemodel.inputs.mask = MNI_mask\n",
    "lmemodel.inputs.subject_dataframe = subject_info\n",
    "lmemodel.inputs.grouping_variable = 'Timepoint'\n",
    "\n",
    "# Mask the file to only significant voxels for clustering\n",
    "mask_stat = Node(Binarize(), name = 'mask_stat')\n",
    "\n",
    "# Cluster the results\n",
    "cluster_results = MapNode(Cluster(threshold=0.95,\n",
    "                                  out_index_file=True,\n",
    "                                  out_localmax_txt_file=True),\n",
    "                          name='cluster_results', \n",
    "                          iterfield = ['in_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LMEManalysisflow = Workflow(name='LMEManalysisflow')\n",
    "LMEManalysisflow.connect([(conditionsource, betamap_grabber, [('condition','condition'),\n",
    "                                                              ('seed','seed')]),\n",
    "                          (betamap_grabber, merge, [('beta_maps','in_files')]),\n",
    "                          (merge, lmemodel, [('merged_file','subject_files')]),\n",
    "                          (merge, datasink, [('merged_file','merged_subj_betas')]),\n",
    "                          (lmemodel, datasink, [('output_volumes','output_volumes')])\n",
    "                         ])\n",
    "#LMEManalysisflow.base_dir = workflow_dir\n",
    "#LMEManalysisflow.write_graph(graph2use='flat')\n",
    "#LMEManalysisflow.run('MultiProc', plugin_args={'n_procs':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_clusters(cluster_index):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "\n",
    "    from os import getcwd\n",
    "    from os.path import abspath\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from numpy import unique, amax, stack, zeros_like\n",
    "\n",
    "    cluster_min = 20\n",
    "    nifti = load(cluster_index)\n",
    "    data = nifti.get_data()\n",
    "    clust_labels, vox_count = unique(data, return_counts=True)\n",
    "    clust_labels = clust_labels.tolist()\n",
    "    vox_count = vox_count.tolist()\n",
    "\n",
    "    iter = 0\n",
    "    for i in range(0, len(clust_labels)):\n",
    "        if vox_count[i] < cluster_min:\n",
    "            del(clust_labels[i-iter])\n",
    "            iter = iter +1\n",
    "\n",
    "    clust_labels = clust_labels[1:]\n",
    "    num_clusters = len(clust_labels)\n",
    "\n",
    "    cluster_masks = []\n",
    "    for a in range(0,num_clusters):\n",
    "        temp = zeros_like(data)\n",
    "        temp[data==clust_labels[a]] = 1\n",
    "        cluster_masks.append(temp)\n",
    "\n",
    "    cluster_masks_4D = stack(cluster_masks,axis=3)\n",
    "    cluster_masks_nifti = Nifti1Image(cluster_masks_4D, nifti.affine)\n",
    "    save(cluster_masks_nifti, 'new_clust_index.nii')\n",
    "    newcluster_index = abspath('new_clust_index.nii')\n",
    "    return(newcluster_index)\n",
    "\n",
    "\n",
    "def finalize_models(cluster, template, betas_text_file, subj_data, model_name):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import abspath\n",
    "    import statsmodels.formula.api as smf\n",
    "    from pandas import DataFrame, Series, concat, read_table, read_csv\n",
    "    from ggplot import *\n",
    "    from warnings import filterwarnings\n",
    "    filterwarnings(\"ignore\")\n",
    "    import sys\n",
    "    from nilearn import plotting\n",
    "    \n",
    "    #determine which model to use\n",
    "    if model_name=='linearAge':\n",
    "        model = 'brain ~ ageMC + sex + ageMC*sex'\n",
    "    elif model_name=='inverseAge':\n",
    "        model = 'brain ~ invAgeMC + sex + invAgeMC*sex'\n",
    "        \n",
    "    origstdout = sys.stdout\n",
    "    sys.stdout = open('modelsummary.txt', 'w')\n",
    "\n",
    "    #organize data into dataframe for modeling\n",
    "    subj_data = read_csv(subj_data, header=0, index_col=0)\n",
    "    brain_data = read_table(betas_text_file, header=None, names='brain', index_col=None)\n",
    "    data = concat([brain_data, subj_data],axis=1)\n",
    "    # do the modeling\n",
    "    mlm = smf.mixedlm(model, data, groups=data[grouping_variable])\n",
    "    mod = mlm.fit()\n",
    "    print(mod.summary())\n",
    "    sys.stdout = origstdout\n",
    "    close('modelsummary.txt')\n",
    "    summary_file = abspath('modelsummary.txt')\n",
    "    \n",
    "    # plot the model results\n",
    "    figure = ggplot(data, aes(x='age',y='brain') + theme_classic() + \n",
    "                    geom_point() + geom_smooth(method=lm,se=True, size=2))\n",
    "    figure.save('plot.svg')\n",
    "    figure_file = abspath('plot.svg')\n",
    "    \n",
    "    # make a picture of the brain cluster\n",
    "    display = plotting.plot_anat(anat_img = template, display_mode='x')\n",
    "    display.add_overlay(cluster, plotting.cm.purple_green, threshold=1)\n",
    "    display.savefig('clusterpic.png')\n",
    "    display.close()\n",
    "    clusterpic_file = abspath('clusterpic.png')\n",
    "    \n",
    "    outputs = [summary_file, figure_file, clusterpic_file]\n",
    "    \n",
    "    return(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data outputs from the LMEMs\n",
    "infosource = Node(IdentityInterface(fields=['condition','seed','term','model']),\n",
    "                  name='conditionsource')\n",
    "infosource.iterables = [('condition', conditions), \n",
    "                        ('seed', seed_names), \n",
    "                        ('term', terms), \n",
    "                        ('model', model_names)]\n",
    "\n",
    "lme_template = {'pval_vol': secondlevel_dir + '/output_volumes/{condition}{seed}/_model_{model}/pval_{term}_data.nii', \n",
    "                'subj_beta_data': secondlevel_dir + '/merged_betas/betas_merged.nii'}            \n",
    "lme_datagrabber = Node(SelectFiles(lme_template), name='lme_datagrabber') \n",
    "\n",
    "# Threshold out nonsignificant voxels\n",
    "threshold = Node(Threshold(thresh=0.95), name='threshold')\n",
    "\n",
    "# Cluster the remaining volumes\n",
    "cluster = Node(Cluster(threshold=0.95, \n",
    "                       out_index_file=True, \n",
    "                       use_mm=True,\n",
    "                       minclustersize=True,\n",
    "                       peak_distance=6), \n",
    "               name='cluster')\n",
    "\n",
    "# remove small clusters\n",
    "cluster_min = Node(Function(input_names=['cluster_index'], \n",
    "                            output_names=['newcluster_index'], \n",
    "                            function=threshold_clusters), \n",
    "                   name='cluster_min')\n",
    "\n",
    "# Split the clusters\n",
    "split = Node(Split(dimension='t'),\n",
    "             name='split')\n",
    "\n",
    "# Extract mean connectivity per cluster\n",
    "pull_mean_betas = MapNode(ImageMeants(out_file='mean_connectivity.txt'), \n",
    "                          name= 'pull_mean_betas', \n",
    "                          iterfield=['mask'])\n",
    "                         \n",
    "\n",
    "# graph the connectivity against age and re-do the linear models\n",
    "finalize_models = MapNode(Function(input_names=['cluster','template','betas_text_file','subj_data', 'model_name'], \n",
    "                                   output_names=['outputs'], \n",
    "                                   function=finalize_models),\n",
    "                          name='finalize_models', \n",
    "                          iterfield=['betas_text_file'])\n",
    "finalize_models.inputs.subj_data=subject_info\n",
    "finalize_models.inputs.template=MNI_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusterflow = Workflow(name='clusterflow')\n",
    "clusterflow.connect([(infosource, lme_datagrabber, [('condition','condition'),\n",
    "                                                    ('seed','seed'),\n",
    "                                                    ('term','term'),\n",
    "                                                    ('model','model')]),\n",
    "                     (lme_datagrabber,threshold, [('pval_vol','in_file')]),\n",
    "                     (threshold, cluster, [('out_file','in_file')]),\n",
    "                     (cluster, cluster_min, [('index_file','cluster_index')]),\n",
    "                     (cluster_min, split, [('newcluster_index','in_file')]),\n",
    "                     (split, pull_mean_betas, [('out_files','mask')]),\n",
    "                     (lme_datagrabber, pull_mean_betas, [('subj_beta_data','in_file')]),\n",
    "                     \n",
    "                     (pull_mean_betas, datasink, [('out_file','beta_values')]),\n",
    "                     (cluster, datasink, [('index_file','cluster_index_file'),\n",
    "                                          ('localmax_txt_file','cluster_localmax_txt_file')]),\n",
    "                     (split,datasink,[('out_files','final_clusters')])\n",
    "                    ])\n",
    "clusterflow.base_dir = workflow_dir\n",
    "clusterflow.write_graph(graph2use='flat')\n",
    "clusterflow.run('MultiProc', plugin_args={'n_procs':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cluster_betas(cluster_file, sample_L1_data, clust_name, cluster_vals):\n",
    "    from pandas import DataFrame, Series, read_csv\n",
    "    from nipype.interfaces.fsl.utils import ImageMeants\n",
    "    \n",
    "    sample_data = read_csv(cluster_vals, index_col=0)\n",
    "\n",
    "    out_prefix = clust_name\n",
    "    \n",
    "    eb = ImageMeants()\n",
    "    eb.inputs.in_file = sample_L1_data\n",
    "    eb.inputs.mask = cluster_file\n",
    "    eb.inputs.out_file = 'L1vals.txt'\n",
    "    eb.run()\n",
    "    L1vals = open('L1vals.txt').read().splitlines()\n",
    "    sample_data[clust_name] = Series(L1vals, index=sample_data.index)\n",
    "    sample_data.to_csv(cluster_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output='/Users/catcamacho/Box/Conferences/20190321_SRCD/talk/1factor/'\n",
    "cluster_vals=output+'clusters_extracted_L1vals.csv'\n",
    "clust_files=[output+'LIPS_ClusterMask_randomise_tstat1_x=4.252129e+01_y=-2.092878e+01_z=3.774994e+01_59voxels.nii', \n",
    "             output+'LITG_ClusterMask_randomise_tstat1_x=4.952129e+01_y=7.123242e-02_z=-2.325006e+01_190voxels.nii', \n",
    "             output+'PSTS_ClusterMask_randomise_tstat1_x=5.052129e+01_y=-2.392877e+01_z=1.274994e+01_72voxels.nii', \n",
    "             output+'RIFG_ClusterMask_randomise_tstat1_x=-5.047871e+01_y=1.207123e+01_z=1.774995e+01_74voxels.nii', \n",
    "             output+'RITG_ClusterMask_randomise_tstat1_x=-2.447871e+01_y=-1.792877e+01_z=-1.425006e+01_109voxels.nii']\n",
    "clust_names=['LIPS','LITG','PSTS','RIFG','RITG']\n",
    "sample_data = output + 'data_merged_smooth.nii.gz'\n",
    "\n",
    "for a in range(0,5):\n",
    "    extract_cluster_betas(clust_files[a], sample_data, clust_names[a], cluster_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "thick_data = read_csv(cluster_vals, index_col=0)\n",
    "full_data = read_csv(output+'all_data_20190320.csv', index_col=0)\n",
    "full_data = full_data.merge(thick_data, on='freesurferID',how='outer')\n",
    "#other_data = read_csv(output+'combined_data_20190311.csv', index_col=0)\n",
    "#vol_data = read_csv(output+'residual_complete_data.csv', index_col=0)\n",
    "#full_data = thick_data.merge(other_data, on='freesurferID',how='outer')\n",
    "#full_data = full_data.merge(vol_data, on='freesurferID',how='outer')\n",
    "full_data.to_csv(output+'all_data_20190320.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data=read_csv(output+'all_data_20190320.csv',index_col=0)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.formula.api import ols\n",
    "screg = ['res_putamen', 'res_pall']\n",
    "region=['LIPS', 'LITG', 'PSTS', 'RIFG', 'RITG']\n",
    "#for a in range(0,2):\n",
    "#    for b in range(0,4):\n",
    "#        print(screg[a]+ ' '+region[b]+' '+str(spearmanr(data[screg[a]],data[region[b]],nan_policy='omit')))\n",
    "#        model = ols('CBCL_externalizing ~ '+region[b]+' + '+region[b]+'*Factor1 + Age_yrs + eTIV + male', \n",
    "#data).fit()\n",
    "#        print(model.summary())\n",
    "        \n",
    "#print(spearmanr(data['PSTS'],data['CBCL_externalizing'],nan_policy='omit'))\n",
    "#print(spearmanr(data['PSTS'],data['Factor1'],nan_policy='omit'))\n",
    "print(spearmanr(data['CBCL_externalizing'],data['res_putamen'],nan_policy='omit'))\n",
    "print(spearmanr(data['CBCL_externalizing'],data['res_pall'],nan_policy='omit'))\n",
    "\n",
    "\n",
    "model = ols('CBCL_externalizing ~ PSTS + PSTS*Factor1 + Age_yrs + eTIV + male', data).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(context='talk',style='white')\n",
    "sns.jointplot('LIPS','Factor1',data=data, kind='reg')\n",
    "plt.savefig(output+'IPL_f1_correlation.svg')\n",
    "from scipy.stats import spearmanr\n",
    "#region=['LIPS', 'LITG', 'PSTS', 'RIFG', 'RITG']\n",
    "#for a in region:\n",
    "#    print(spearmanr(data[a],data['CBCL_externalizing'],nan_policy='omit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_assign(c):\n",
    "    if c['Factor1']<=0:\n",
    "        return 'low'\n",
    "    elif c['Factor1']>0:\n",
    "        return 'high'\n",
    "sns.set(context='talk', style='white')\n",
    "\n",
    "data['irr_group'] = data.apply(group_assign, axis=1) \n",
    "sns.lmplot(x='PSTS',y='CBCL_externalizing',hue='irr_group', data=data, ci=None)\n",
    "plt.savefig(output+'irr_PSTS_interact.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "males= data[data['male']==1]\n",
    "females=data[data['male']==0]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist([males['Age_yrs'],females['Age_yrs']], bins=10, stacked=True, \n",
    "         label=['Male','Female'])\n",
    "plt.legend()\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.savefig(output + 'age_sex_hist.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = data[data['seq1']==1]\n",
    "seq2 = data[data['seq2']==1]\n",
    "seq3 = data[data['seq3']==1]\n",
    "seq4 = data[data['seq4']==1]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist([seq1['Age_yrs'],seq2['Age_yrs'],seq3['Age_yrs'],seq4['Age_yrs']],bins=10,\n",
    "         stacked=True, label=['version 1', 'version 2','version 3','version 4'])\n",
    "plt.legend()\n",
    "plt.xlabel('Age (Years)')\n",
    "plt.savefig(output + 'age_sequence_hist.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
